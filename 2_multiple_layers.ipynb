{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building Intuition about NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(([3,5],[5,1],[10,2]),dtype = float) # two features, hours studies and hours slept\n",
    "y = np.array(([75],[82],[93]),dtype = float) # the mark the student got\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3.,  5.],\n",
       "       [ 5.,  1.],\n",
       "       [10.,  2.]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's look at the features matrix\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 2)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[75.],\n",
       "       [82.],\n",
       "       [93.]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# and the response variable\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 1)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 1, scaling the data, and since all the values are positives then we can just devide by the max\n",
    "\n",
    "X = X/np.max(X, axis = 0)\n",
    "y = y/100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.3, 1. ],\n",
       "       [0.5, 0.2],\n",
       "       [1. , 0.4]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.75],\n",
       "       [0.82],\n",
       "       [0.93]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# well I again got the correct answer, I should get some time to think about this mistery?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN_model:\n",
    "    def __init__(self):\n",
    "        \n",
    "        # here we will define the size of the layer as the number of the neurons/units in each layer\n",
    "        np.random.seed(500)\n",
    "        self.input_layer_size = 2\n",
    "        self.hidden_layer_size = 3\n",
    "        self.output_layer_sizer = 1\n",
    "        # weights\n",
    "        \n",
    "        # note that the weight matrix shape deponds only on the size of layers and not on the number of example, we only need X[1] shape and not X[0]     \n",
    "        self.w1 = np.random.rand(self.input_layer_size,self.hidden_layer_size)\n",
    "        self.w2 = np.random.rand(self.hidden_layer_size,self.output_layer_sizer)\n",
    "        \n",
    "\n",
    "    \n",
    "    def sigmoid(self,z):\n",
    "        return 1/(1+np.exp(-z))\n",
    "\n",
    "    def sigmoid_grad(self,z):\n",
    "        return self.sigmoid(z) * (1-self.sigmoid(z))\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self,X):\n",
    "        \"\"\"\n",
    "        moving the data through the network, here we will pass the inputs X1,X2,... as matrix X instead of inividuals units \n",
    "        \"\"\"\n",
    "        \n",
    "        # we will name the hidden layers by the letter Z, since the first hidden layer is also the second layer in the nn then we will start with z2\n",
    "        \n",
    "        z2 = X@self.w1\n",
    "        # we apply activation function\n",
    "        print('z2')\n",
    "        print(z2)\n",
    "        \n",
    "        a2 = self.sigmoid(z2)  \n",
    "        print('a2')\n",
    "        print(a2)\n",
    "        \n",
    "        z3 = a2@self.w2\n",
    "        print('z3')\n",
    "        print(z3)\n",
    "        \n",
    "        a3 = self.sigmoid(z3)\n",
    "        \n",
    "        # since a3 is also y_hat then\n",
    "        \n",
    "        y_hat = a3\n",
    "        \n",
    "\n",
    "        \n",
    "        return y_hat\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's test the forward function for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z2\n",
      "[[0.76731279 0.10362572 0.59240254]\n",
      " [0.45868155 0.04788062 0.41178961]\n",
      " [0.9173631  0.09576124 0.82357922]]\n",
      "a2\n",
      "[[0.68293931 0.52588327 0.64391621]\n",
      " [0.61270136 0.51196787 0.60151692]\n",
      " [0.71450451 0.52392203 0.69499558]]\n",
      "z3\n",
      "[[0.7345827 ]\n",
      " [0.6805321 ]\n",
      " [0.76710807]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.67581011],\n",
       "       [0.66385745],\n",
       "       [0.68289498]])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm = NN_model()\n",
    "lm.forward(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# why all the predictions are the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN_model:\n",
    "    def __init__(self):\n",
    "        \n",
    "        # here we will define the size of the layer as the number of the neurons/units in each layer\n",
    "        self.input_layer_size = 2 # we need to include those in the input of the functions as parameterss\n",
    "        self.hidden_layer_size = 3\n",
    "        self.output_layer_sizer = 1\n",
    "        \n",
    "        \n",
    "        # weights\n",
    "        \n",
    "        # note that the weight matrix shape deponds only on the size of layers and not on the number of example, we only need X[1] shape and not X[0]     \n",
    "        self.w1 = np.random.rand(self.input_layer_size,self.hidden_layer_size)\n",
    "        self.w2 = np.random.rand(self.hidden_layer_size,self.output_layer_sizer)\n",
    "        \n",
    "\n",
    "    \n",
    "    def sigmoid(self,z):\n",
    "        return 1/(1+np.exp(-z))\n",
    "\n",
    "    def sigmoid_grad(self,z):\n",
    "        return self.sigmoid(z) * (1-self.sigmoid(z))\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self,X):\n",
    "        \"\"\"\n",
    "        moving the data through the network, here we will pass the inputs X1,X2,... as matrix X instead of inividuals units \n",
    "        \"\"\"\n",
    "        \n",
    "        # we will name the hidden layers by the letter Z, since the first hidden layer is also the second layer in the nn then we will start with z2\n",
    "        \n",
    "        z2 = X@self.w1\n",
    "        # we apply activation function\n",
    "        \n",
    "        a2 = self.sigmoid(z2)\n",
    "        \n",
    "        z3 = a2@self.w2\n",
    "        \n",
    "        a3 = self.sigmoid(z3)\n",
    "        \n",
    "        # \n",
    "        \n",
    "        y_hat = a3\n",
    "        \n",
    "        return y_hat\n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    " \n",
    "    \n",
    "    \n",
    "    def calc_loss(self,X,y,w1,w2 ,loss_measure = 'MSE'):\n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "        Calculating the loss or the deviation of the results we got from the model compared with the real values. The first step is to calculate the X@w which represents the results we got from\n",
    "        the model. The second step is to evaulate the loss which can be done through a loss function that we define. for example here we define two ways to caculate the loss either using MAE, in\n",
    "        which we first calculate the absolute difference between each predicted and true value and then we calucalte the mean. or using the MSW, in which we calculate the abolute difference between\n",
    "        true and preidcted value, square it and then find the average. The loss we then be returned.\n",
    "        \n",
    "        \"\"\"    \n",
    "        losses = []\n",
    "        \n",
    "        # the first layer in the model, will take the inputs X, which has two units x1 and x2 and produce z1, z1 will have two units, z1_1, and z2_2\n",
    "        z1 = X@w1\n",
    "        print('z1')\n",
    "        print(z1)\n",
    "        # the output layer\n",
    "        y_predicted= z1@w2        \n",
    "        #y_predicted = self.sigmoid(z2)\n",
    "        print('y_predicted',y_predicted)\n",
    "        #print('grad', self.sigmoid(z) * (1-self.sigmoid(z)))\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        if loss_measure == 'MSE':\n",
    "            loss = (y - y_predicted)**2\n",
    "        elif loss_measure == 'MAE':\n",
    "            loss= abs(y - y_predicted)\n",
    "\n",
    "        #print('the true value = ',y)\n",
    "        #print('the predicted value = ',y_predicted)\n",
    "        losses.append(loss)\n",
    "        total_loss = np.mean(losses)\n",
    "        #print('the total loss =', total_loss )\n",
    "        return total_loss\n",
    "      \n",
    "\n",
    "    def diff(self,w,X,y,loss_measure,diff_type = 'Analytical'):\n",
    "        \"\"\"\n",
    "        given a function f(x) and a point w, calculate th f'(x)\n",
    "        \n",
    "        \"\"\"\n",
    "        dldw = np.ones([X.shape[1],1])\n",
    "        h = 0.005  \n",
    "        \n",
    "        if diff_type =='Numerical':\n",
    "            # calculate the derivitives dl/dw using limits\n",
    "            for i in range(len(w)):\n",
    "                print('i',i)\n",
    "                # f(w)\n",
    "                loss_w = self.calc_loss(X,y,w,loss_measure = loss_measure)\n",
    "                # we will choose the margin that we will accept as 0.5\n",
    "                # f(w+h)\n",
    "                w[i] = w[i] + h      \n",
    "                loss_w_plus_h = self.calc_loss(X,y,w,loss_measure = loss_measure)\n",
    "                # dldw = f(w+h) - f(w)/h\n",
    "                dldw[i] = (loss_w_plus_h- loss_w)/h\n",
    "                print('dldw using numerical solution')\n",
    "                print(dldw)\n",
    "        elif diff_type == 'Analytical':\n",
    "            # calculate the derivitves dl/dw using calculus\n",
    "            # l = (X.w - y)**2\n",
    "            # dl/dw = 2*(X.w - y)@X = 2*X@()\n",
    "            \n",
    "            z = X@w\n",
    "            \n",
    "            print('sigmpod_grad = ',self.sigmoid_grad(z))\n",
    "            \n",
    "            dldw = 2*X@(z - y)#*self.sigmoid_grad(z)\n",
    "            print('dldw using analytical solution')\n",
    "            print(dldw)\n",
    "        return dldw \n",
    "    \n",
    "    def train(self,X,y,loss_measure = 'MAE', lr = 0.07, diff_type = 'Analytical'):\n",
    "        \"\"\"\n",
    "        training of the model is done by evaluting loss and updating the weights\n",
    "        \"\"\"\n",
    "        # initilization\n",
    "        np.random.seed(50)\n",
    "        w1 = np.random.rand(X.shape[1],2)\n",
    "        w2 = np.random.rand(X.shape[1],1)\n",
    "        # intialization. I am changing this manually\n",
    "        #w[0] = \n",
    "        #w[1] = 3.5\n",
    "        #print('initial weights')\n",
    "        #print(w1)    \n",
    "        # updating the weight     \n",
    "        #lr =0.07\n",
    "        \n",
    "        for i in range(30):\n",
    "            print('step =',i)\n",
    "            \n",
    "            # forward pass is essentially the process of calculting the loss given the data and the weight, for the loss we need to calculate a predicted value\n",
    "            # calculate the loss at the current weights\n",
    "            loss_w = self.calc_loss(X,y,w1,w2)\n",
    "            print('loss is now',loss_w)\n",
    "            # stoping condition, note that we don't require the loss is to be exactly zero but close enough\n",
    "            if loss_w <= 0.08 :\n",
    "                self.w = w\n",
    "                print('final w = ',self.w)\n",
    "                return self.w\n",
    "            \n",
    "            # updating the weight. \n",
    "            elif loss_w > 0.08 :\n",
    "                # the backpropagation is essentially the calculation of the partially derivives using chain rules. since we have only one layer at this time we will just calculate the dldw\n",
    "                #First at that particular point we calculate dldw \n",
    "                \n",
    "                \n",
    "                # dldw1 = dl/dz*dz/dw1\n",
    "                #dl/dz =2(x@w-y), l = (z-y)**2\n",
    "                \n",
    "                #dz/dw = X>>>>>>>>>>>>>why: becauaw z =X@w\n",
    "         \n",
    "                # the new w is w2\n",
    "                \n",
    "                z1 = X@w1 # this is the new x\n",
    "                dldw2 =  2*z1@(z1@w2 - y)#self.diff(w1,X,y,loss_measure, diff_type)\n",
    "                \n",
    "                print('x')\n",
    "                print(X)\n",
    "                print('w2')\n",
    "                print(w2)\n",
    "                \n",
    "#                 h= X@w2\n",
    "#                 print(h)\n",
    "#                 h2 = z1@w2  -y\n",
    "#                 print(h2)\n",
    "                \n",
    "#                 dldw1 = 2*(h)@(h2)\n",
    "                \n",
    "                \n",
    "#                 \n",
    "                \n",
    "                dldw1 = 2*w2*(z1@w2-y)@X\n",
    "                               \n",
    "                               \n",
    "                # dl/dw1\n",
    "                print('dldw1,2')\n",
    "                print(dldw1)\n",
    "                print(dldw2)\n",
    "                \n",
    "               # print(w2)\n",
    "                #dldw2 = self.diff(w2,X,y,loss_measure, diff_type)\n",
    "                # Now we update the weight\n",
    "                w1 = w1 -lr*dldw1\n",
    "                w2 = w2 -lr*dldw2\n",
    "                \n",
    "                print('w1')\n",
    "                print(w1)  \n",
    "                                \n",
    "                print('w2')\n",
    "                print(w2) \n",
    "                \n",
    "    def predict(self,X_new):\n",
    "        \"\"\"\n",
    "        Given a new data, the model will be using the weights stored in the self.w to produce a prediction\n",
    "        \"\"\"\n",
    "        predicted_y = X_new@self.w\n",
    "        \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

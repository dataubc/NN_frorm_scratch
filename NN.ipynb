{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building Intuision about NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The best approach to get an intuision for building NN is to start from scratch, and I will start with matrix mutiplication, as it's the foundational knowledge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's start with equations, after all linear algebra was built to solve equations\n",
    "\n",
    "X.w = y\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what does this eqaution tells us, that the dot product of feature matrix X and weight vector w will get us y. But the shape matters for matrix multiplication as we will see\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's creat our feature matrix, we will have a data with only one example and 5 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[10, 12,  5,  3, -5]])"
      ]
     },
     "execution_count": 346,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array([10,12,5,3,-5]).reshape(1,5)\n",
    "X\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we have come to know that there is a dependent variable y, that is linearly dependent on those features. That means if we take the right mix of theres features in X we will get y.\n",
    " of course the million dollar question is that what is the right mix. Let's say we the right mix of feature X1 is w1 and from featureX2 is w2 etc. Now, we will get the following equation\n",
    "\n",
    "X1.w1 + X2.w2 + ...... Xn.wn = y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can write the above equation in a compact form in the following form:\n",
    "\n",
    "X.w = y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, to write this equation in the matrix form, we need to know the rules of matrix multiplication\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- the rows of w has to be the same as the columns of X\n",
    "- so if X shape is 1,5 then w needs to be 5x, so we can have any numbe of columns but the rows have to be 5. The resuled matrix will take the shape of (rows of X , columns of w)\n",
    "- why is this important, well, since the number of rows in X is the numbe of example, observations in the data does not affect. We can have the same shape of w for any number of observations. \n",
    "- On the other hands, w is affected by the number of features, of course becuase each w is connected with aparticualr feature.\n",
    "- Now when it comes the resulted y, of coursee we need one y for each observation, and therefore the numbe of rows in y is the same as number of observation. Now let's say we don't have one output\n",
    "- I mean we have y1 and y2, we will face this in multi-classificaiton and also in NN when w have a layer with more than one unit. Here th columns of y will increase, and guess who will increase the numbe of columns to accomodate this, yes that's right. That's the NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ok, we need to learn about the dot product, so we can peformr X.w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X is 5 * 1 and the output is 2 *1 so we need the w to be 2*5\n",
    "\n",
    "w = np.array([\n",
    "    [ 0.1, 0.1, 0.1,0.1,0.1],\n",
    "    [0.1,0.1,0.1,0.1,0.1]\n",
    "\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = w.reshape(5,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.1, 0.1],\n",
       "       [0.1, 0.1],\n",
       "       [0.1, 0.1],\n",
       "       [0.1, 0.1],\n",
       "       [0.1, 0.1]])"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.5, 1.5]])"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X@w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input layer, 5 nodes        layer 1, 2 nodes\n",
    "\n",
    "#\n",
    "#                    #\n",
    "#\n",
    "#                    #\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's make up a simple example to see if the NN can actually learn from examples\n",
    "\n",
    "\n",
    "# y X1 + X2 + X3 + X4 + X5\n",
    "\n",
    "# examples\n",
    "X = np.array([[1,1,1,1,1],\n",
    "             [2,1,2,2,2],\n",
    "            [3,1,3,3,3],\n",
    "            [3,2,3,3,3],\n",
    "            [3,3,3,3,3],\n",
    "            [3,4,3,3,3],\n",
    "            [3,5,3,3,3],\n",
    "            [3,6,3,3,3],\n",
    "            [3,7,3,3,3],\n",
    "            [3,8,3,3,3]\n",
    "             ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 5)"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10,)"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = np.array([5,9,13,14,15,16,17,18,19,20])\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "class linear_model:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def train(self,X,y):\n",
    "        # create the weight vectorr\n",
    "        w = np.ones([X.shape[1],1])\n",
    "        losses = []\n",
    "        y_predicted = np.zeros(len(X))\n",
    "        for i in range(len(X)):\n",
    "            y_predicted[i] = X[i]@w\n",
    "            loss = y[i] - y_predicted[i]\n",
    "            \n",
    "            print('the true value = ',y[i])\n",
    "            print('the predicted value = ',y_predicted[i])\n",
    "            losses.append(loss)\n",
    "        total_loss = np.sum(losses)\n",
    "        print('the total loss =', total_loss )\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = linear_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the true value =  5\n",
      "the predicted value =  5.0\n",
      "the true value =  9\n",
      "the predicted value =  9.0\n",
      "the true value =  13\n",
      "the predicted value =  13.0\n",
      "the true value =  14\n",
      "the predicted value =  14.0\n",
      "the true value =  15\n",
      "the predicted value =  15.0\n",
      "the true value =  16\n",
      "the predicted value =  16.0\n",
      "the true value =  17\n",
      "the predicted value =  17.0\n",
      "the true value =  18\n",
      "the predicted value =  18.0\n",
      "the true value =  19\n",
      "the predicted value =  19.0\n",
      "the true value =  20\n",
      "the predicted value =  20.0\n",
      "the total loss = 0.0\n"
     ]
    }
   ],
   "source": [
    "lm.train(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note that we used the correct w for this case and hence we have loss of zero, let's change ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "class linear_model:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def train(self,X,y):\n",
    "        w = np.ones([X.shape[1],1])\n",
    "        w[0] = 0.2\n",
    "        losses = []\n",
    "        y_predicted = np.zeros(len(X))\n",
    "        for i in range(len(X)):\n",
    "            y_predicted[i] = X[i]@w\n",
    "            loss_at_w = y[i] - y_predicted[i]\n",
    "            loss_at_w_plus_h = y[i] - y_predicted[i]\n",
    "            \n",
    "            print('the true value = ',y[i])\n",
    "            print('the predicted value = ',y_predicted[i])\n",
    "            losses.append(loss_at_w )\n",
    "        total_loss = np.sum(losses)\n",
    "        print('the total loss =', total_loss)\n",
    "              \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = linear_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the true value =  5\n",
      "the predicted value =  4.2\n",
      "the true value =  9\n",
      "the predicted value =  7.4\n",
      "the true value =  13\n",
      "the predicted value =  10.6\n",
      "the true value =  14\n",
      "the predicted value =  11.6\n",
      "the true value =  15\n",
      "the predicted value =  12.6\n",
      "the true value =  16\n",
      "the predicted value =  13.6\n",
      "the true value =  17\n",
      "the predicted value =  14.6\n",
      "the true value =  18\n",
      "the predicted value =  15.6\n",
      "the true value =  19\n",
      "the predicted value =  16.6\n",
      "the true value =  20\n",
      "the predicted value =  17.6\n",
      "the total loss = 21.599999999999998\n"
     ]
    }
   ],
   "source": [
    "lm.train(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as you can see here, the loss is no longer 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it is obvious that we need to keep tweeking ws until we get the minium loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### now comes the calculs part, first we know that we can write the loss as a function of ws\n",
    "\n",
    "- L = f(w)\n",
    "- dl/dw = f'(w) = (f(w+h) - f(w))/h\n",
    "\n",
    "new_w = old+w - step_size* dl/dw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    " #ok, I need to update w which is a vector of shape (x[1],1)\n",
    "        # any change in w, for example if we change w[0], the vector y will change completely, not only y[0] as we dicussed before.\n",
    "        # now as we said we want to change w by tweeking it a little bit, by adding dw, i.e adding a vector that will be similar in length to w\n",
    "              \n",
    "        # for example :\n",
    "              # w = [1               df/dw = [0.10                                                        [1.2\n",
    "              #      2                     0.04            > new w will be old_w _ step_size*dw.     >. 2.08         note that here we assumed step*size to be 2\n",
    "               #     3]                    0.03]                                                         2.06\n",
    "        \n",
    "        # let's calculate dl/dw\n",
    "        # but this will involve calculating the loss twice, one at and then at w + h\n",
    "        # let's make a function of calculating the loss\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using a simple model to solve the equation 3X = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "class linear_model:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def calc_loss(self,X,y,w):\n",
    "       \n",
    "        losses = []\n",
    "        y_predicted = np.zeros(len(X))\n",
    "       \n",
    "        \n",
    "        y_predicted = X@w\n",
    "        loss= y - y_predicted\n",
    "            \n",
    "        print('the true value = ',y)\n",
    "        print('the predicted value = ',y_predicted)\n",
    "        losses.append(loss)\n",
    "        total_loss = np.sum(losses)\n",
    "        print('the total loss =', total_loss )\n",
    "        return total_loss\n",
    "      \n",
    "\n",
    "\n",
    "    def train(self,X,y):\n",
    "        w = np.ones([X.shape[1],1])\n",
    "        w[0] = 5\n",
    "        \n",
    "        for i in range(len(w)):\n",
    "            loss_w = self.calc_loss(X,y,w)\n",
    "            \n",
    "            # the case when the loss is positive, in this case we need to decrease w\n",
    "            \n",
    "            # we will choose the margin that we will accept as 0.5\n",
    "            \n",
    "            if abs(loss_w)> 0:\n",
    "                # since the equation is 3x =6 , then dw/dl = 3\n",
    "                dldw = 3\n",
    "            elif abs(loss_w)< 0:\n",
    "                dldw = -3\n",
    "                # we will make the the learning rate is 0.5\n",
    "            lr =1\n",
    "            w[0]= w[0] -lr*dldw\n",
    "            loss_w_dash= self.calc_loss(X,y,w)\n",
    "            print(loss_w_dash - loss_w)\n",
    "            loss_w = loss_w_dash\n",
    "\n",
    "            \n",
    "            self.w = w\n",
    "            print(self.w)\n",
    "        \n",
    "    def predict(self,X_new):\n",
    "        predicted_y = X_new@self.w\n",
    "        \n",
    "        return predicted_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([3]).reshape(1,1)\n",
    "y = np.array([6]).reshape(1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1)"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.ones([1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.]])"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X@w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = linear_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the true value =  [[6]]\n",
      "the predicted value =  [[15.]]\n",
      "the total loss = -9.0\n",
      "the true value =  [[6]]\n",
      "the predicted value =  [[6.]]\n",
      "the total loss = 0.0\n",
      "9.0\n",
      "[[2.]]\n"
     ]
    }
   ],
   "source": [
    "lm.train(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "# so essentially the equation we have is\n",
    "\n",
    "# 2*X, this is our model, because this is basically what got us 6\n",
    "# let's see what if we try a new X, say for example 4, we expect the new y to be 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = np.array([4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = lm.predict(X_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([8.])"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### let's now try equations for 2 variables\n",
    "\n",
    "\n",
    "- X1 = 2\n",
    "- X2 = 3\n",
    "\n",
    "- 2X1 + X2 = 7\n",
    "- 3X1 - X2 = 3\n",
    "\n",
    "> this is the equivelant of having 2 examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2,  1],\n",
       "       [ 3, -1]])"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array([[2,1],\n",
    "             [3,-1]])\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[7],\n",
       "       [3]])"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = np.array([7,3]).reshape(2,1)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "class linear_model:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def calc_loss(self,X,y,w):\n",
    "       \n",
    "        losses = []\n",
    "        y_predicted = np.zeros(len(X))\n",
    "       \n",
    "        \n",
    "        y_predicted = X@w\n",
    "        loss= y - y_predicted\n",
    "\n",
    "        print('the true value = ',y)\n",
    "        print('the predicted value = ',y_predicted)\n",
    "        losses.append(loss)\n",
    "        total_loss = np.sum(losses)\n",
    "        print('the total loss =', total_loss )\n",
    "        return total_loss\n",
    "      \n",
    "\n",
    "\n",
    "    def train(self,X,y):\n",
    "        w = np.ones([X.shape[1],1])\n",
    "        w[0] = 5\n",
    "        w[1] = 3\n",
    "        \n",
    "        loss_w = self.calc_loss(X,y,w)\n",
    "        while abs(loss_w) > 1:\n",
    "            for i in range(len(w)):\n",
    "                loss_w = self.calc_loss(X,y,w)\n",
    "\n",
    "                # the case when the loss is positive, in this case we need to decrease w\n",
    "\n",
    "                # we will choose the margin that we will accept as 0.5\n",
    "\n",
    "                if abs(loss_w)> 0:\n",
    "                    # since the equation is 3x =6 , then dw/dl = 3\n",
    "                    dldw = 1\n",
    "                elif abs(loss_w)< 0:\n",
    "                    dldw = -3\n",
    "                    # we will make the the learning rate is 0.5\n",
    "                lr =1\n",
    "                w[i]= w[i] -lr*dldw\n",
    "                loss_w_dash= self.calc_loss(X,y,w)\n",
    "                print(loss_w_dash - loss_w)\n",
    "                loss_w = loss_w_dash\n",
    "            \n",
    "            if abs(loss_w) < 5:\n",
    "                \n",
    "                self.w = w\n",
    "                \n",
    "                return self.w\n",
    "                print('converged to solution')\n",
    "                \n",
    "            else:\n",
    "                print('not converged')\n",
    "            \n",
    "            \n",
    "\n",
    "    def predict(self,X_new):\n",
    "        predicted_y = X_new@self.w\n",
    "        \n",
    "        return predicted_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = linear_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the true value =  [[7]\n",
      " [3]]\n",
      "the predicted value =  [[13.]\n",
      " [12.]]\n",
      "the total loss = -15.0\n",
      "the true value =  [[7]\n",
      " [3]]\n",
      "the predicted value =  [[13.]\n",
      " [12.]]\n",
      "the total loss = -15.0\n",
      "the true value =  [[7]\n",
      " [3]]\n",
      "the predicted value =  [[11.]\n",
      " [ 9.]]\n",
      "the total loss = -10.0\n",
      "5.0\n",
      "the true value =  [[7]\n",
      " [3]]\n",
      "the predicted value =  [[11.]\n",
      " [ 9.]]\n",
      "the total loss = -10.0\n",
      "the true value =  [[7]\n",
      " [3]]\n",
      "the predicted value =  [[10.]\n",
      " [10.]]\n",
      "the total loss = -10.0\n",
      "0.0\n",
      "not vonverged\n",
      "the true value =  [[7]\n",
      " [3]]\n",
      "the predicted value =  [[10.]\n",
      " [10.]]\n",
      "the total loss = -10.0\n",
      "the true value =  [[7]\n",
      " [3]]\n",
      "the predicted value =  [[8.]\n",
      " [7.]]\n",
      "the total loss = -5.0\n",
      "5.0\n",
      "the true value =  [[7]\n",
      " [3]]\n",
      "the predicted value =  [[8.]\n",
      " [7.]]\n",
      "the total loss = -5.0\n",
      "the true value =  [[7]\n",
      " [3]]\n",
      "the predicted value =  [[7.]\n",
      " [8.]]\n",
      "the total loss = -5.0\n",
      "0.0\n",
      "not vonverged\n",
      "the true value =  [[7]\n",
      " [3]]\n",
      "the predicted value =  [[7.]\n",
      " [8.]]\n",
      "the total loss = -5.0\n",
      "the true value =  [[7]\n",
      " [3]]\n",
      "the predicted value =  [[5.]\n",
      " [5.]]\n",
      "the total loss = 0.0\n",
      "5.0\n",
      "the true value =  [[7]\n",
      " [3]]\n",
      "the predicted value =  [[5.]\n",
      " [5.]]\n",
      "the total loss = 0.0\n",
      "the true value =  [[7]\n",
      " [3]]\n",
      "the predicted value =  [[4.]\n",
      " [6.]]\n",
      "the total loss = 0.0\n",
      "0.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[2.],\n",
       "       [0.]])"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.train(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "# even though this is not the correct value, but the tota loss is actually zero. I need to to solve this proble,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "#n the issue of losses cancelling themselves are a direct result of not taking the sum of the absolute value , which is L1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "# L1 loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "class linear_model:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def calc_loss(self,X,y,w):\n",
    "        \n",
    "        # just X@w matrix multiplicaion and then calculating the differrence from the real y and returning the absoute value.\n",
    "       \n",
    "        losses = []\n",
    "        y_predicted = np.zeros(len(X))\n",
    "       \n",
    "        \n",
    "        y_predicted = X@w\n",
    "        loss= abs(y - y_predicted)\n",
    "\n",
    "        #print('the true value = ',y)\n",
    "        #print('the predicted value = ',y_predicted)\n",
    "        losses.append(loss)\n",
    "        total_loss = np.sum(losses)\n",
    "        #print('the total loss =', total_loss )\n",
    "        return total_loss\n",
    "      \n",
    "\n",
    "\n",
    "    def train(self,X,y):\n",
    "        w = np.ones([X.shape[1],1])\n",
    "        dldw = np.ones([X.shape[1],1])\n",
    "        dldw[0] = 0.02\n",
    "        dldw[1] = 0.02\n",
    "        w[0] = 5\n",
    "        w[1] = 3\n",
    "        \n",
    "    # calculate the derivitives, this happen once, that was my mistake, I was including it in the for loop.\n",
    "\n",
    "        for i in range(len(w)):\n",
    "\n",
    "            # f(w)\n",
    "\n",
    "            loss_w = self.calc_loss(X,y,w)\n",
    "            # we will choose the margin that we will accept as 0.5\n",
    "\n",
    "            # f(w+h)\n",
    "\n",
    "            h = 0.005\n",
    "            w[i] = w[i] + h\n",
    "\n",
    "            # dldw = f(w+h) - f(w)/h\n",
    "            dldw[i] = (self.calc_loss(X,y,w) - loss_w)/h\n",
    "\n",
    "\n",
    "        print(dldw)   \n",
    "        \n",
    "        # updating the weight\n",
    "        \n",
    "        lr =0.1\n",
    "        \n",
    "        for i in range(10):\n",
    "            loss_w = self.calc_loss(X,y,w)\n",
    "            print('loss is now',loss_w)\n",
    "            if loss_w <= 0.05 :\n",
    "                self.w = w\n",
    "                print('final w = ',self.w)\n",
    "            elif loss_w > 0.5 :\n",
    "                w = w - lr*dldw\n",
    "#         for i in range(10):\n",
    "#             counter = counter + 1\n",
    "            \n",
    "#             print('loss = ',loss_w_dash)\n",
    "#             #print(loss_w_dash - loss_w)\n",
    "#             loss_w = loss_w_dash\n",
    "\n",
    "#             \n",
    "\n",
    "#                 return self.w\n",
    "                \n",
    "                \n",
    "\n",
    "    def predict(self,X_new):\n",
    "        predicted_y = X_new@self.w\n",
    "        \n",
    "        return predicted_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = linear_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5.00000000e+00]\n",
      " [3.55271368e-13]]\n",
      "loss is now 15.025000000000002\n",
      "loss is now 12.524999999999967\n",
      "loss is now 10.024999999999931\n",
      "loss is now 7.524999999999895\n",
      "loss is now 5.024999999999859\n",
      "loss is now 2.5249999999998227\n",
      "loss is now 0.024999999999786304\n",
      "final w =  [[2.005]\n",
      " [3.005]]\n",
      "loss is now 0.024999999999786304\n",
      "final w =  [[2.005]\n",
      " [3.005]]\n",
      "loss is now 0.024999999999786304\n",
      "final w =  [[2.005]\n",
      " [3.005]]\n",
      "loss is now 0.024999999999786304\n",
      "final w =  [[2.005]\n",
      " [3.005]]\n"
     ]
    }
   ],
   "source": [
    "lm.train(X,y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# wa hoooooooooo :)\n",
    "\n",
    "- The breakthrough was the fact hat I have to calculate dwdl once\n",
    "- Also making the condition of stopping not zero but at 0.05\n",
    "- And playing with the learning rate\n",
    "- Of course also th earlier realization that I need to calucalte L1 and not just sum th loss since they were cancelling each other."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

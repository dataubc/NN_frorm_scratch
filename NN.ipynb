{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building Intuition about NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 775,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 776,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The best approach to get an intuision for building NN is to start from scratch, and I will start with matrix mutiplication, as it's the foundational knowledge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's start with equations, after all linear algebra was built to solve equations\n",
    "\n",
    "X.w = y\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 777,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what does this eqaution tells us, that the dot product of feature matrix X and weight vector w will get us y. But the shape matters for matrix multiplication as we will see\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 778,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's creat our feature matrix, we will have a data with only one example and 5 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 779,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[10, 12,  5,  3, -5]])"
      ]
     },
     "execution_count": 779,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array([10,12,5,3,-5]).reshape(1,5)\n",
    "X\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we have come to know that there is a dependent variable y, that is linearly dependent on those features. That means if we take the right mix of theres features in X we will get y.\n",
    " of course the million dollar question is that what is the right mix. Let's say we the right mix of feature X1 is w1 and from featureX2 is w2 etc. Now, we will get the following equation\n",
    "\n",
    "X1.w1 + X2.w2 + ...... Xn.wn = y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can write the above equation in a compact form in the following form:\n",
    "\n",
    "X.w = y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, to write this equation in the matrix form, we need to know the rules of matrix multiplication\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- the rows of w has to be the same as the columns of X\n",
    "- so if X shape is 1,5 then w needs to be 5x, so we can have any numbe of columns but the rows have to be 5. The resuled matrix will take the shape of (rows of X , columns of w)\n",
    "- why is this important, well, since the number of rows in X is the numbe of example, observations in the data does not affect. We can have the same shape of w for any number of observations. \n",
    "- On the other hands, w is affected by the number of features, of course becuase each w is connected with aparticualr feature.\n",
    "- Now when it comes the resulted y, of coursee we need one y for each observation, and therefore the numbe of rows in y is the same as number of observation. Now let's say we don't have one output\n",
    "- I mean we have y1 and y2, we will face this in multi-classificaiton and also in NN when w have a layer with more than one unit. Here th columns of y will increase, and guess who will increase the numbe of columns to accomodate this, yes that's right. That's the NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 780,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ok, we need to learn about the dot product, so we can peformr X.w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 781,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X is 5 * 1 and the output is 2 *1 so we need the w to be 2*5\n",
    "\n",
    "w = np.array([\n",
    "    [ 0.1, 0.1, 0.1,0.1,0.1],\n",
    "    [0.1,0.1,0.1,0.1,0.1]\n",
    "\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 782,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = w.reshape(5,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 783,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.1, 0.1],\n",
       "       [0.1, 0.1],\n",
       "       [0.1, 0.1],\n",
       "       [0.1, 0.1],\n",
       "       [0.1, 0.1]])"
      ]
     },
     "execution_count": 783,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 784,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.5, 2.5]])"
      ]
     },
     "execution_count": 784,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X@w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 785,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input layer, 5 nodes        layer 1, 2 nodes\n",
    "\n",
    "#\n",
    "#                    #\n",
    "#\n",
    "#                    #\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 786,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's make up a simple example to see if the NN can actually learn from examples\n",
    "\n",
    "\n",
    "# y X1 + X2 + X3 + X4 + X5\n",
    "\n",
    "# examples\n",
    "X = np.array([[1,1,1,1,1],\n",
    "             [2,1,2,2,2],\n",
    "            [3,1,3,3,3],\n",
    "            [3,2,3,3,3],\n",
    "            [3,3,3,3,3],\n",
    "            [3,4,3,3,3],\n",
    "            [3,5,3,3,3],\n",
    "            [3,6,3,3,3],\n",
    "            [3,7,3,3,3],\n",
    "            [3,8,3,3,3]\n",
    "             ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 787,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 5)"
      ]
     },
     "execution_count": 787,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 788,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10,)"
      ]
     },
     "execution_count": 788,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = np.array([5,9,13,14,15,16,17,18,19,20])\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 789,
   "metadata": {},
   "outputs": [],
   "source": [
    "class linear_model:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def train(self,X,y):\n",
    "        # create the weight vectorr\n",
    "        w = np.ones([X.shape[1],1])\n",
    "        losses = []\n",
    "        y_predicted = np.zeros(len(X))\n",
    "        for i in range(len(X)):\n",
    "            y_predicted[i] = X[i]@w\n",
    "            loss = y[i] - y_predicted[i]\n",
    "            \n",
    "            print('the true value = ',y[i])\n",
    "            print('the predicted value = ',y_predicted[i])\n",
    "            losses.append(loss)\n",
    "        total_loss = np.sum(losses)\n",
    "        print('the total loss =', total_loss )\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 790,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = linear_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 791,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the true value =  5\n",
      "the predicted value =  5.0\n",
      "the true value =  9\n",
      "the predicted value =  9.0\n",
      "the true value =  13\n",
      "the predicted value =  13.0\n",
      "the true value =  14\n",
      "the predicted value =  14.0\n",
      "the true value =  15\n",
      "the predicted value =  15.0\n",
      "the true value =  16\n",
      "the predicted value =  16.0\n",
      "the true value =  17\n",
      "the predicted value =  17.0\n",
      "the true value =  18\n",
      "the predicted value =  18.0\n",
      "the true value =  19\n",
      "the predicted value =  19.0\n",
      "the true value =  20\n",
      "the predicted value =  20.0\n",
      "the total loss = 0.0\n"
     ]
    }
   ],
   "source": [
    "lm.train(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 792,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 792,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 793,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note that we used the correct w for this case and hence we have loss of zero, let's change ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 794,
   "metadata": {},
   "outputs": [],
   "source": [
    "class linear_model:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def train(self,X,y):\n",
    "        w = np.ones([X.shape[1],1])\n",
    "        w[0] = 0.2\n",
    "        losses = []\n",
    "        y_predicted = np.zeros(len(X))\n",
    "        for i in range(len(X)):\n",
    "            y_predicted[i] = X[i]@w\n",
    "            loss_at_w = y[i] - y_predicted[i]\n",
    "            loss_at_w_plus_h = y[i] - y_predicted[i]\n",
    "            \n",
    "            print('the true value = ',y[i])\n",
    "            print('the predicted value = ',y_predicted[i])\n",
    "            losses.append(loss_at_w )\n",
    "        total_loss = np.sum(losses)\n",
    "        print('the total loss =', total_loss)\n",
    "              \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 795,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = linear_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 796,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the true value =  5\n",
      "the predicted value =  4.2\n",
      "the true value =  9\n",
      "the predicted value =  7.4\n",
      "the true value =  13\n",
      "the predicted value =  10.6\n",
      "the true value =  14\n",
      "the predicted value =  11.6\n",
      "the true value =  15\n",
      "the predicted value =  12.6\n",
      "the true value =  16\n",
      "the predicted value =  13.6\n",
      "the true value =  17\n",
      "the predicted value =  14.6\n",
      "the true value =  18\n",
      "the predicted value =  15.6\n",
      "the true value =  19\n",
      "the predicted value =  16.6\n",
      "the true value =  20\n",
      "the predicted value =  17.6\n",
      "the total loss = 21.599999999999998\n"
     ]
    }
   ],
   "source": [
    "lm.train(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 797,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as you can see here, the loss is no longer 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 798,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it is obvious that we need to keep tweeking ws until we get the minium loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### now comes the calculs part, first we know that we can write the loss as a function of ws\n",
    "\n",
    "- L = f(w)\n",
    "- dl/dw = f'(w) = (f(w+h) - f(w))/h\n",
    "\n",
    "new_w = old+w - step_size* dl/dw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 799,
   "metadata": {},
   "outputs": [],
   "source": [
    " #ok, I need to update w which is a vector of shape (x[1],1)\n",
    "        # any change in w, for example if we change w[0], the vector y will change completely, not only y[0] as we dicussed before.\n",
    "        # now as we said we want to change w by tweeking it a little bit, by adding dw, i.e adding a vector that will be similar in length to w\n",
    "              \n",
    "        # for example :\n",
    "              # w = [1               df/dw = [0.10                                                        [1.2\n",
    "              #      2                     0.04            > new w will be old_w _ step_size*dw.     >. 2.08         note that here we assumed step*size to be 2\n",
    "               #     3]                    0.03]                                                         2.06\n",
    "        \n",
    "        # let's calculate dl/dw\n",
    "        # but this will involve calculating the loss twice, one at and then at w + h\n",
    "        # let's make a function of calculating the loss\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using a simple model to solve the equation 3X = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 800,
   "metadata": {},
   "outputs": [],
   "source": [
    "class linear_model:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def calc_loss(self,X,y,w):\n",
    "       \n",
    "        losses = []\n",
    "        y_predicted = np.zeros(len(X))\n",
    "       \n",
    "        \n",
    "        y_predicted = X@w\n",
    "        loss= y - y_predicted\n",
    "            \n",
    "        print('the true value = ',y)\n",
    "        print('the predicted value = ',y_predicted)\n",
    "        losses.append(loss)\n",
    "        total_loss = np.sum(losses)\n",
    "        print('the total loss =', total_loss )\n",
    "        return total_loss\n",
    "      \n",
    "\n",
    "\n",
    "    def train(self,X,y):\n",
    "        w = np.ones([X.shape[1],1])\n",
    "        w[0] = 5\n",
    "        \n",
    "        for i in range(len(w)):\n",
    "            loss_w = self.calc_loss(X,y,w)\n",
    "            \n",
    "            # the case when the loss is positive, in this case we need to decrease w\n",
    "            \n",
    "            # we will choose the margin that we will accept as 0.5\n",
    "            \n",
    "            if abs(loss_w)> 0:\n",
    "                # since the equation is 3x =6 , then dw/dl = 3\n",
    "                dldw = 3\n",
    "            elif abs(loss_w)< 0:\n",
    "                dldw = -3\n",
    "                # we will make the the learning rate is 0.5\n",
    "            lr =1\n",
    "            w[0]= w[0] -lr*dldw\n",
    "            loss_w_dash= self.calc_loss(X,y,w)\n",
    "            print(loss_w_dash - loss_w)\n",
    "            loss_w = loss_w_dash\n",
    "\n",
    "            \n",
    "            self.w = w\n",
    "            print(self.w)\n",
    "        \n",
    "    def predict(self,X_new):\n",
    "        predicted_y = X_new@self.w\n",
    "        \n",
    "        return predicted_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 801,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([3]).reshape(1,1)\n",
    "y = np.array([6]).reshape(1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 802,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1)"
      ]
     },
     "execution_count": 802,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 803,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.ones([1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 804,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.]])"
      ]
     },
     "execution_count": 804,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X@w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 805,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = linear_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 806,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the true value =  [[6]]\n",
      "the predicted value =  [[15.]]\n",
      "the total loss = -9.0\n",
      "the true value =  [[6]]\n",
      "the predicted value =  [[6.]]\n",
      "the total loss = 0.0\n",
      "9.0\n",
      "[[2.]]\n"
     ]
    }
   ],
   "source": [
    "lm.train(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 807,
   "metadata": {},
   "outputs": [],
   "source": [
    "# so essentially the equation we have is\n",
    "\n",
    "# 2*X, this is our model, because this is basically what got us 6\n",
    "# let's see what if we try a new X, say for example 4, we expect the new y to be 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 808,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = np.array([4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 809,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = lm.predict(X_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 810,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([8.])"
      ]
     },
     "execution_count": 810,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### let's now try equations for 2 variables\n",
    "\n",
    "\n",
    "- X1 = 2\n",
    "- X2 = 3\n",
    "\n",
    "- 2X1 + X2 = 7\n",
    "- 3X1 - X2 = 3\n",
    "\n",
    "> this is the equivelant of having 2 examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 811,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2,  1],\n",
       "       [ 3, -1]])"
      ]
     },
     "execution_count": 811,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array([[2,1],\n",
    "             [3,-1]])\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 812,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[7],\n",
       "       [3]])"
      ]
     },
     "execution_count": 812,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = np.array([7,3]).reshape(2,1)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 813,
   "metadata": {},
   "outputs": [],
   "source": [
    "class linear_model:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def calc_loss(self,X,y,w):\n",
    "       \n",
    "        losses = []\n",
    "        y_predicted = np.zeros(len(X))\n",
    "       \n",
    "        \n",
    "        y_predicted = X@w\n",
    "        loss= y - y_predicted\n",
    "\n",
    "        print('the true value = ',y)\n",
    "        print('the predicted value = ',y_predicted)\n",
    "        losses.append(loss)\n",
    "        total_loss = np.sum(losses)\n",
    "        print('the total loss =', total_loss )\n",
    "        return total_loss\n",
    "      \n",
    "\n",
    "\n",
    "    def train(self,X,y):\n",
    "        w = np.ones([X.shape[1],1])\n",
    "        w[0] = 5\n",
    "        w[1] = 3\n",
    "        \n",
    "        loss_w = self.calc_loss(X,y,w)\n",
    "        while abs(loss_w) > 1:\n",
    "            for i in range(len(w)):\n",
    "                loss_w = self.calc_loss(X,y,w)\n",
    "\n",
    "                # the case when the loss is positive, in this case we need to decrease w\n",
    "\n",
    "                # we will choose the margin that we will accept as 0.5\n",
    "\n",
    "                if abs(loss_w)> 0:\n",
    "                    # since the equation is 3x =6 , then dw/dl = 3\n",
    "                    dldw = 1\n",
    "                elif abs(loss_w)< 0:\n",
    "                    dldw = -3\n",
    "                    # we will make the the learning rate is 0.5\n",
    "                lr =1\n",
    "                w[i]= w[i] -lr*dldw\n",
    "                loss_w_dash= self.calc_loss(X,y,w)\n",
    "                print(loss_w_dash - loss_w)\n",
    "                loss_w = loss_w_dash\n",
    "            \n",
    "            if abs(loss_w) < 5:\n",
    "                \n",
    "                self.w = w\n",
    "                \n",
    "                return self.w\n",
    "                print('converged to solution')\n",
    "                \n",
    "            else:\n",
    "                print('not converged')\n",
    "            \n",
    "            \n",
    "\n",
    "    def predict(self,X_new):\n",
    "        predicted_y = X_new@self.w\n",
    "        \n",
    "        return predicted_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 814,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = linear_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 815,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the true value =  [[7]\n",
      " [3]]\n",
      "the predicted value =  [[13.]\n",
      " [12.]]\n",
      "the total loss = -15.0\n",
      "the true value =  [[7]\n",
      " [3]]\n",
      "the predicted value =  [[13.]\n",
      " [12.]]\n",
      "the total loss = -15.0\n",
      "the true value =  [[7]\n",
      " [3]]\n",
      "the predicted value =  [[11.]\n",
      " [ 9.]]\n",
      "the total loss = -10.0\n",
      "5.0\n",
      "the true value =  [[7]\n",
      " [3]]\n",
      "the predicted value =  [[11.]\n",
      " [ 9.]]\n",
      "the total loss = -10.0\n",
      "the true value =  [[7]\n",
      " [3]]\n",
      "the predicted value =  [[10.]\n",
      " [10.]]\n",
      "the total loss = -10.0\n",
      "0.0\n",
      "not converged\n",
      "the true value =  [[7]\n",
      " [3]]\n",
      "the predicted value =  [[10.]\n",
      " [10.]]\n",
      "the total loss = -10.0\n",
      "the true value =  [[7]\n",
      " [3]]\n",
      "the predicted value =  [[8.]\n",
      " [7.]]\n",
      "the total loss = -5.0\n",
      "5.0\n",
      "the true value =  [[7]\n",
      " [3]]\n",
      "the predicted value =  [[8.]\n",
      " [7.]]\n",
      "the total loss = -5.0\n",
      "the true value =  [[7]\n",
      " [3]]\n",
      "the predicted value =  [[7.]\n",
      " [8.]]\n",
      "the total loss = -5.0\n",
      "0.0\n",
      "not converged\n",
      "the true value =  [[7]\n",
      " [3]]\n",
      "the predicted value =  [[7.]\n",
      " [8.]]\n",
      "the total loss = -5.0\n",
      "the true value =  [[7]\n",
      " [3]]\n",
      "the predicted value =  [[5.]\n",
      " [5.]]\n",
      "the total loss = 0.0\n",
      "5.0\n",
      "the true value =  [[7]\n",
      " [3]]\n",
      "the predicted value =  [[5.]\n",
      " [5.]]\n",
      "the total loss = 0.0\n",
      "the true value =  [[7]\n",
      " [3]]\n",
      "the predicted value =  [[4.]\n",
      " [6.]]\n",
      "the total loss = 0.0\n",
      "0.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[2.],\n",
       "       [0.]])"
      ]
     },
     "execution_count": 815,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.train(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 816,
   "metadata": {},
   "outputs": [],
   "source": [
    "# even though this is not the correct value, but the tota loss is actually zero. I need to to solve this proble,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 817,
   "metadata": {},
   "outputs": [],
   "source": [
    "#n the issue of losses cancelling themselves are a direct result of not taking the sum of the absolute value , which is L1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 818,
   "metadata": {},
   "outputs": [],
   "source": [
    "# L1 loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 819,
   "metadata": {},
   "outputs": [],
   "source": [
    "class linear_model:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "  \n",
    "        \n",
    "        return predicted_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 820,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = linear_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 821,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.50000000e+00]\n",
      " [1.77635684e-13]]\n",
      "step 0\n",
      "loss is now 7.512500000000001\n",
      "step 1\n",
      "loss is now 6.887499999999992\n",
      "step 2\n",
      "loss is now 6.262499999999983\n",
      "step 3\n",
      "loss is now 5.637499999999974\n",
      "step 4\n",
      "loss is now 5.0124999999999655\n",
      "step 5\n",
      "loss is now 4.387499999999957\n",
      "step 6\n",
      "loss is now 3.7624999999999473\n",
      "step 7\n",
      "loss is now 3.1374999999999384\n",
      "step 8\n",
      "loss is now 2.5124999999999296\n",
      "step 9\n",
      "loss is now 1.8874999999999207\n",
      "step 10\n",
      "loss is now 1.2624999999999114\n",
      "step 11\n",
      "loss is now 0.637499999999902\n",
      "step 12\n",
      "loss is now 0.012499999999893152\n",
      "final w =  [[2.005]\n",
      " [3.005]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[2.005],\n",
       "       [3.005]])"
      ]
     },
     "execution_count": 821,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.train(X,y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# wa hoooooooooo :)\n",
    "\n",
    "- The breakthrough was the fact hat I have to calculate dwdl once\n",
    "- Also making the condition of stopping not zero but at 0.05\n",
    "- And playing with the learning rate\n",
    "- Of course also th earlier realization that I need to calucalte L1 and not just sum th loss since they were cancelling each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 822,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch vs stochastic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "while I was sleeping yesterday I was thinking about the calculation of gradients. More specifically I was thinking about how it is possible to calculate the gradient using one example, using some examples or all examples,or using all examples. The only difference will be in the way the loss is calculated. If only one example is included then we cacluate one loss, if n the the loss vectors, will have n rows each is associated with a loss, of course since we need only one number we will just take the mean of all losses. Note that we are calculatin the mean squared error using this methodology MAE. If we use L2. then we will be calculting the MSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 823,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 950,
   "metadata": {},
   "outputs": [],
   "source": [
    "class linear_model:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def calc_loss(self,X,y,w, loss_measure = 'MSE'):\n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "        Calculating the loss or the deviation of the results we got from the model compared with the real values. The first step is to calculate the X@w which represents the results we got from\n",
    "        the model. The second step is to evaulate the loss which can be done through a loss function that we define. for example here we define two ways to caculate the loss either using MAE, in\n",
    "        which we first calculate the absolute difference between each predicted and true value and then we calucalte the mean. or using the MSW, in which we calculate the abolute difference between\n",
    "        true and preidcted value, square it and then find the average. The loss we then be returned.\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "       \n",
    "        losses = []\n",
    "        y_predicted = np.zeros(len(X))\n",
    "        y_predicted = X@w\n",
    "        \n",
    "        if loss_measure == 'MSE':\n",
    "            loss = (y - y_predicted)**2\n",
    "        elif loss_measure == 'MAE':\n",
    "            loss= abs(y - y_predicted)\n",
    "\n",
    "        #print('the true value = ',y)\n",
    "        #print('the predicted value = ',y_predicted)\n",
    "        losses.append(loss)\n",
    "        total_loss = np.mean(losses)\n",
    "        #print('the total loss =', total_loss )\n",
    "        return total_loss\n",
    "      \n",
    "\n",
    "\n",
    "    def train(self,X,y,loss_measure = 'MAE'):\n",
    "        #w = np.ones([X.shape[1],1])\n",
    "        np.random.seed(500)\n",
    "        w = np.random.rand(X.shape[1],1)\n",
    "        dldw = np.ones([X.shape[1],1])\n",
    "        dldw[0] = 0.02\n",
    "        dldw[1] = 0.02\n",
    "        w[0] = 3\n",
    "        w[1] = 3\n",
    "        print(w)\n",
    "        \n",
    "    # calculate the derivitives, this happen once, that was my mistake, I was including it in the for loop.\n",
    "\n",
    "        for i in range(len(w)):\n",
    "\n",
    "            # f(w)\n",
    "\n",
    "            loss_w = self.calc_loss(X,y,w,loss_measure = loss_measure)\n",
    "            # we will choose the margin that we will accept as 0.5\n",
    "\n",
    "            # f(w+h)\n",
    "\n",
    "            h = 0.005\n",
    "            w[i] = w[i] + h\n",
    "\n",
    "            # dldw = f(w+h) - f(w)/h\n",
    "            dldw[i] = (self.calc_loss(X,y,w,loss_measure = loss_measure) - loss_w)/h\n",
    "\n",
    "\n",
    "        print(dldw)   \n",
    "        \n",
    "        # updating the weight\n",
    "        \n",
    "        lr =0.005\n",
    "        \n",
    "        for i in range(20):\n",
    "            print('step',i)\n",
    "            loss_w = self.calc_loss(X,y,w)\n",
    "            print('loss is now',loss_w)\n",
    "            if loss_w <= 0.08 :\n",
    "                self.w = w\n",
    "                print('final w = ',self.w)\n",
    "                return self.w\n",
    "            elif loss_w > 0.08 :\n",
    "                w = w - lr*dldw\n",
    "#         for i in range(10):\n",
    "#             counter = counter + 1\n",
    "            \n",
    "#             print('loss = ',loss_w_dash)\n",
    "#             #print(loss_w_dash - loss_w)\n",
    "#             loss_w = loss_w_dash\n",
    "\n",
    "#             \n",
    "\n",
    "#                 return self.w\n",
    "                \n",
    "                \n",
    "\n",
    "    def predict(self,X_new):\n",
    "        predicted_y = X_new@self.w\n",
    "        \n",
    "        return predicted_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 951,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3.]\n",
      " [3.]]\n",
      "[[13.0325]\n",
      " [-1.    ]]\n",
      "step 0\n",
      "loss is now 6.560162500000003\n",
      "step 1\n",
      "loss is now 5.7321160466406065\n",
      "step 2\n",
      "loss is now 4.9599711865624725\n",
      "step 3\n",
      "loss is now 4.243727919765584\n",
      "step 4\n",
      "loss is now 3.5833862462499555\n",
      "step 5\n",
      "loss is now 2.978946166015569\n",
      "step 6\n",
      "loss is now 2.4304076790624403\n",
      "step 7\n",
      "loss is now 1.9377707853905637\n",
      "step 8\n",
      "loss is now 1.501035484999939\n",
      "step 9\n",
      "loss is now 1.1202017778905646\n",
      "step 10\n",
      "loss is now 0.7952696640624431\n",
      "step 11\n",
      "loss is now 0.5262391435155735\n",
      "step 12\n",
      "loss is now 0.31311021624995605\n",
      "step 13\n",
      "loss is now 0.15588288226559058\n",
      "step 14\n",
      "loss is now 0.05455714156247709\n",
      "final w =  [[2.092725]\n",
      " [3.075   ]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[2.092725],\n",
       "       [3.075   ]])"
      ]
     },
     "execution_count": 951,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm = linear_model()\n",
    "lm.train(X,y,loss_measure = 'MSE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 901,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_2d_new = np.array([[2,3],\n",
    "                   [0,5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 855,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[13.845125],\n",
       "       [16.15    ]])"
      ]
     },
     "execution_count": 855,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.predict(X_2d_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 828,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's use the model we have trained for predicting the results for new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The effect of initialzing weighhts with very smalll values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 939,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ok, we know that dldw is basically how much the loss is affected when the weight is changed a little bit. Since the trajectory is not linear\n",
    "# we expect that dldw is not constant. But I basically makes it so in the above code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think now I started to grasp the idea of foward pass and backward pass.\n",
    "Where do we need the dldw, we need it when we want to update w. Now dldw is not constatnt because again , imagin that you are in this half-circle \n",
    "down. as you apprach the bottom the slopes acually goes dowm. I am going to do this for the above code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1034,
   "metadata": {},
   "outputs": [],
   "source": [
    "class linear_model:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def calc_loss(self,X,y,w, loss_measure = 'MSE'):\n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "        Calculating the loss or the deviation of the results we got from the model compared with the real values. The first step is to calculate the X@w which represents the results we got from\n",
    "        the model. The second step is to evaulate the loss which can be done through a loss function that we define. for example here we define two ways to caculate the loss either using MAE, in\n",
    "        which we first calculate the absolute difference between each predicted and true value and then we calucalte the mean. or using the MSW, in which we calculate the abolute difference between\n",
    "        true and preidcted value, square it and then find the average. The loss we then be returned.\n",
    "        \n",
    "        \"\"\"    \n",
    "        losses = []\n",
    "        y_predicted = np.zeros(len(X))\n",
    "        y_predicted = X@w\n",
    "        \n",
    "        if loss_measure == 'MSE':\n",
    "            loss = (y - y_predicted)**2\n",
    "        elif loss_measure == 'MAE':\n",
    "            loss= abs(y - y_predicted)\n",
    "\n",
    "        #print('the true value = ',y)\n",
    "        #print('the predicted value = ',y_predicted)\n",
    "        losses.append(loss)\n",
    "        total_loss = np.mean(losses)\n",
    "        #print('the total loss =', total_loss )\n",
    "        return total_loss\n",
    "      \n",
    "\n",
    "    def diff(self,w,X,y, loss_measure):\n",
    "        \"\"\"\n",
    "        given a function f(x) and a point w, calculate th f'(x)\n",
    "        \n",
    "        \"\"\"\n",
    "        dldw = np.ones([X.shape[1],1])\n",
    "        h = 0.005  \n",
    "        for i in range(len(w)):\n",
    "            print('i',i)\n",
    "\n",
    "            # f(w)\n",
    "            loss_w = self.calc_loss(X,y,w,loss_measure = loss_measure)\n",
    "            # we will choose the margin that we will accept as 0.5\n",
    "            # f(w+h)\n",
    "            w[i] = w[i] + h      \n",
    "            loss_w_plus_h = self.calc_loss(X,y,w,loss_measure = loss_measure)\n",
    "            # dldw = f(w+h) - f(w)/h\n",
    "            dldw[i] = (loss_w_plus_h- loss_w)/h\n",
    "            \n",
    "            print('dldw')\n",
    "            print(dldw)\n",
    "            \n",
    "        return dldw  \n",
    "        \n",
    "     \n",
    "    # calculate the derivitives, this happen once, that was my mistake, I was including it in the for loop.\n",
    "    def train(self,X,y,loss_measure = 'MAE', lr = 0.07):\n",
    "        \"\"\"\n",
    "        training of the model is done by evaluting loss and updating the weights\n",
    "        \"\"\"\n",
    "        # initilization\n",
    "        np.random.seed(50)\n",
    "        w = np.random.rand(X.shape[1],1)\n",
    "        # intialization. I am changing this manually\n",
    "        #w[0] = \n",
    "        #w[1] = 3.5\n",
    "        print(\n",
    "        )\n",
    "        print(w)    \n",
    "        # updating the weight     \n",
    "        #lr =0.07\n",
    "        \n",
    "        for i in range(20):\n",
    "            print('step =',i)\n",
    "            \n",
    "            # forward pass is essentially the process of calculting the loss given the data and the weight, for the loss we need to calculate a predicted value\n",
    "            # calculate the loss at the current weights\n",
    "            loss_w = self.calc_loss(X,y,w)\n",
    "            print('loss is now',loss_w)\n",
    "            # stoping condition, note that we don't require the loss is to be exactly zero but close enough\n",
    "            if loss_w <= 0.08 :\n",
    "                self.w = w\n",
    "                print('final w = ',self.w)\n",
    "                return self.w\n",
    "            \n",
    "            # updating the weight. \n",
    "            elif loss_w > 0.08 :\n",
    "                # the backpropagation is essentially the calculation of the partially derivives using chain rules. since we have only one layer at this time we will just calculate the dldw\n",
    "                #First at that particular point we calculate dldw \n",
    "                dldw = self.diff(w,X,y,loss_measure)\n",
    "                # Now we update the weight\n",
    "                w = w -lr*dldw\n",
    "                print('w')\n",
    "                print(w)          \n",
    "    def predict(self,X_new):\n",
    "        predicted_y = X_new@self.w\n",
    "        \n",
    "        return predicted_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1035,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.49460165]\n",
      " [0.2280831 ]]\n",
      "step = 0\n",
      "loss is now 18.241141479081318\n",
      "i 0\n",
      "dldw\n",
      "[[-16.76576171]\n",
      " [  1.        ]]\n",
      "i 1\n",
      "dldw\n",
      "[[-16.76576171]\n",
      " [ -4.03843544]]\n",
      "w\n",
      "[[2.17617782]\n",
      " [0.63692665]]\n",
      "step = 1\n",
      "loss is now 6.202187820754634\n",
      "i 0\n",
      "dldw\n",
      "[[4.68588497]\n",
      " [1.        ]]\n",
      "i 1\n",
      "dldw\n",
      "[[ 4.68588497]\n",
      " [-4.90232452]]\n",
      "w\n",
      "[[1.71258932]\n",
      " [1.1321591 ]]\n",
      "step = 2\n",
      "loss is now 3.488924047704765\n",
      "i 0\n",
      "dldw\n",
      "[[-1.83599794]\n",
      " [ 1.        ]]\n",
      "i 1\n",
      "dldw\n",
      "[[-1.83599794]\n",
      " [-3.44827112]]\n",
      "w\n",
      "[[1.90118911]\n",
      " [1.48198621]]\n",
      "step = 3\n",
      "loss is now 2.217832915642432\n",
      "i 0\n",
      "dldw\n",
      "[[0.26597227]\n",
      " [1.        ]]\n",
      "i 1\n",
      "dldw\n",
      "[[ 0.26597227]\n",
      " [-2.93721669]]\n",
      "w\n",
      "[[1.87959189]\n",
      " [1.78070788]]\n",
      "step = 4\n",
      "loss is now 1.4340983468824953\n",
      "i 0\n",
      "dldw\n",
      "[[-0.31351335]\n",
      " [ 1.        ]]\n",
      "i 1\n",
      "dldw\n",
      "[[-0.31351335]\n",
      " [-2.31817612]]\n",
      "w\n",
      "[[1.91594322]\n",
      " [2.01752549]]\n",
      "step = 5\n",
      "loss is now 0.9285985368217826\n",
      "i 0\n",
      "dldw\n",
      "[[-0.07776361]\n",
      " [ 1.        ]]\n",
      "i 1\n",
      "dldw\n",
      "[[-0.07776361]\n",
      " [-1.88089223]]\n",
      "w\n",
      "[[1.92871958]\n",
      " [2.21061472]]\n",
      "step = 6\n",
      "loss is now 0.5998872488193161\n",
      "i 0\n",
      "dldw\n",
      "[[-0.10476014]\n",
      " [ 1.        ]]\n",
      "i 1\n",
      "dldw\n",
      "[[-0.10476014]\n",
      " [-1.50749015]]\n",
      "w\n",
      "[[1.9441956 ]\n",
      " [2.36636373]]\n",
      "step = 7\n",
      "loss is now 0.3863770806328031\n",
      "i 0\n",
      "dldw\n",
      "[[-0.05932097]\n",
      " [ 1.        ]]\n",
      "i 1\n",
      "dldw\n",
      "[[-0.05932097]\n",
      " [-1.21146813]]\n",
      "w\n",
      "[[1.95512769]\n",
      " [2.49251055]]\n",
      "step = 8\n",
      "loss is now 0.24786122958527262\n",
      "i 0\n",
      "dldw\n",
      "[[-0.04335052]\n",
      " [ 1.        ]]\n",
      "i 1\n",
      "dldw\n",
      "[[-0.04335052]\n",
      " [-0.9701066 ]]\n",
      "w\n",
      "[[1.96446275]\n",
      " [2.59452121]]\n",
      "step = 9\n",
      "loss is now 0.15821227652901654\n",
      "i 0\n",
      "dldw\n",
      "[[-0.0240055]\n",
      " [ 1.       ]]\n",
      "i 1\n",
      "dldw\n",
      "[[-0.0240055 ]\n",
      " [-0.77542034]]\n",
      "w\n",
      "[[1.9718633 ]\n",
      " [2.67706324]]\n",
      "step = 10\n",
      "loss is now 0.10034765718420702\n",
      "i 0\n",
      "dldw\n",
      "[[-0.01034038]\n",
      " [ 1.        ]]\n",
      "i 1\n",
      "dldw\n",
      "[[-0.01034038]\n",
      " [-0.61773682]]\n",
      "w\n",
      "[[1.97789733]\n",
      " [2.74383692]]\n",
      "step = 11\n",
      "loss is now 0.06313306704950684\n",
      "final w =  [[1.97789733]\n",
      " [2.74383692]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1.97789733],\n",
       "       [2.74383692]])"
      ]
     },
     "execution_count": 1035,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm = linear_model()\n",
    "lm.train(X,y,loss_measure = 'MSE', lr = 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- note that weight in the first dimention X[0] is being updated correctly, when I initialize with weight that is bigger than the correct once, the weight goes down, and when I initialize with \n",
    "a weight that is smaller than the correct weight then the weight will go up. But, for some reason there is almost no movement in the second dimention, X[1], the quetion is why ?I alreay tried initializing with a different set of values, but still there is no movements.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- I guess the solution deponds on how X1 and X2 affect f(x) and hence the loss funciton. It seems that if we change w a bit in X1 direction, the loss will be affected significantly, but if we \n",
    "change w in the X2 direction, then the loss will be affected that much hence the slow conversion to the correct values of w."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building Intuition about NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dot product"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best approach to get an intuition for building NN is to start from scratch, and I will start with matrix mutiplication, as it's the foundational corner stone.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, I will start with the concept of dot product. There are multiple useful ways to think about dot product as following:\n",
    "-  1 The weighted average. For example the cummlative GPA of a student who study for 5 years could be calucalted by dot producting the weight vector with the gpa vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.3, 3.1, 2.9, 3.4, 3.5])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpa = np.array([3.3,3.1,2.9,3.4,3.5])\n",
    "gpa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.06666667, 0.13333333, 0.2       , 0.26666667, 0.33333333])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = np.array([1/15,2/15,3/15,4/15,5/15])\n",
    "weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We want: 1/15 * 0.066 + 2/15 * 0.1333 + 3.15 * 0.2 + ...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "essentially to multiply each gpa with the asssocaited weght. To do this, we need the first rule in matrix multiplication, which dictates the shape of each vector. if we are performing A@B, and A shape is M * N , B shape has to be N * K, which is to say that the rows of B has to match the columns of A."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5,)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's check the shape of gpa\n",
    "gpa.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.3, 3.1, 2.9, 3.4, 3.5]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we need to reshape gpa to be 1*5\n",
    "\n",
    "gpa = gpa.reshape(1,5)\n",
    "gpa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5,)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's check the shape of weights\n",
    "weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.06666667],\n",
       "       [0.13333333],\n",
       "       [0.2       ],\n",
       "       [0.26666667],\n",
       "       [0.33333333]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we need to reshape it to be 5*1\n",
    "\n",
    "weights = weights.reshape(5,1)\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.28666667]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now to get the dot product of gpa and weights we use the @ symbol\n",
    "\n",
    "gpa@weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " > and this will be the final results for the student."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's say we have more than one student. We can put the results of that student in the second column of gpa matrix. Note that the number of rows in gpa matrix don't require any change in the shape of the weights vectos. Additionally note that the change in the results will be reflected in the number of rows and not in the number of columns. Let's test this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.3, 3.1, 2.9, 3.4, 3.5],\n",
       "       [3.1, 2.5, 2.6, 2.8, 2.9]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first, let's create the record of student b and then add it to the gpa matrix\n",
    "student_b = np.array([3.1,2.5,2.6,2.8,2.9]).reshape(1,5)\n",
    "gpa = np.vstack([gpa,student_b])\n",
    "gpa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.28666667],\n",
       "       [2.77333333]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now let's get the results for both students\n",
    "gpa@weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# multi-output\n",
    "We saw how changing the rows or the columns of X change the output. But how about chaning the type of outputs that we want. In out previous examle we had one target which was the gpa, what if we want to change the number of outpouts to be more than one, say for example we want to have the average of grades in addition to the gpa. In this case we need to include another column in the weight matrix to connect with the average. So essentially the number of columns in `w` should be the same as the number of outputs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.06666667, 0.2       ],\n",
       "       [0.13333333, 0.2       ],\n",
       "       [0.2       , 0.2       ],\n",
       "       [0.26666667, 0.2       ],\n",
       "       [0.33333333, 0.2       ]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = np.hstack([weights,np.array([1/5,1/5,1/5,1/5,1/5]).reshape(5,1)])\n",
    "weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for each of the two students we have, we will calculate the avg in addition to the gpa\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.28666667, 3.24      ],\n",
       "       [2.77333333, 2.78      ]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpa@weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">The first student, has a cummulative gpa of 3.28 and avg of 3.24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This knowledge will come handy when we are dealing with NN that has many hidden layer with many units.\n",
    "# for example let's say we have the seoncd layer with 10 units, and the third layer 5 units, then the w matrix will be 10 by 5, i,e 10 rows and 5 columns.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 2: The concept of dot product as a similarity measure. While this could not be of obvius value here, thinking about dot product as a similarity measure is very useful. Here we will be thiking of two vector that origiated form the zero and live in the 2D plane. Say vecor A is\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[0 3] and nother vector B which is [3 0]. It's obvious that A lives in the x-axis while B lives in the y-axis. The dot product between those vectors is zero reflecting that A and B are basically totally different. Now, if we consider another vector C [0 2.9], we can see clearly that C is very similar to vector A. Indeed the dot product will be much bigger between A and C than between A and B."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solving linear equation system "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building on what we now know about dot product let's try to solve the following equation:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X.w = y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "what does this eqaution tells us is that the dot product of feature matrix `X` and weight vector `w` will get us the response vector `y`. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's creat our feature matrix, we will have a data with only one example and 5 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[10, 12,  5,  3, -5]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array([10,12,5,3,-5]).reshape(1,5)\n",
    "X\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have come to know that there is a dependent variable `y`, that is linearly dependent on those features. That means if we take the right mix of theres features in `X` we will get y.\n",
    " of course the million dollar question is that what is the right mix. Let's say we the right mix of feature X1 is w1 and from featureX2 is w2 etc. Now, we will get the following equation\n",
    "\n",
    "X1.w1 + X2.w2 + ...... Xn.wn = y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can write the above equation in a compact form in the following form:\n",
    "\n",
    "X.w = y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, to write this equation in the matrix form, we need to know the rules of matrix multiplication\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- the rows of w has to be the same as the columns of X\n",
    "- so if X shape is 1,5 then w needs to be 5x, so we can have any numbe of columns but the rows have to be 5. The resuled matrix will take the shape of (rows of X , columns of w)\n",
    "- why is this important, well, since the number of rows in X is the numbe of example, observations in the data does not affect. We can have the same shape of w for any number of observations. \n",
    "- On the other hands, w is affected by the number of features, of course becuase each w is connected with aparticualr feature.\n",
    "- Now when it comes the resulted y, of coursee we need one y for each observation, and therefore the numbe of rows in y is the same as number of observation. Now let's say we don't have one output\n",
    "- I mean we have y1 and y2, we will face this in multi-classificaiton and also in NN when w have a layer with more than one unit. Here th columns of y will increase, and guess who will increase the numbe of columns to accomodate this, yes that's right. That's the NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ok, we need to learn about the dot product, so we can peformr X.w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X is 5 * 1 and the output is 2 *1 so we need the w to be 2*5\n",
    "\n",
    "w = np.array([\n",
    "    [ 0.1, 0.1, 0.1,0.1,0.1],\n",
    "    [0.1,0.1,0.1,0.1,0.1]\n",
    "\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = w.reshape(5,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.1, 0.1],\n",
       "       [0.1, 0.1],\n",
       "       [0.1, 0.1],\n",
       "       [0.1, 0.1],\n",
       "       [0.1, 0.1]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.5, 2.5]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X@w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input layer, 5 nodes        layer 1, 2 nodes\n",
    "\n",
    "#\n",
    "#                    #\n",
    "#\n",
    "#                    #\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's make up a simple example to see if the NN can actually learn from examples\n",
    "\n",
    "\n",
    "# y X1 + X2 + X3 + X4 + X5\n",
    "\n",
    "# examples\n",
    "X = np.array([[1,1,1,1,1],\n",
    "             [2,1,2,2,2],\n",
    "            [3,1,3,3,3],\n",
    "            [3,2,3,3,3],\n",
    "            [3,3,3,3,3],\n",
    "            [3,4,3,3,3],\n",
    "            [3,5,3,3,3],\n",
    "            [3,6,3,3,3],\n",
    "            [3,7,3,3,3],\n",
    "            [3,8,3,3,3]\n",
    "             ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 5)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10,)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = np.array([5,9,13,14,15,16,17,18,19,20])\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class linear_model:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def train(self,X,y):\n",
    "        # create the weight vectorr\n",
    "        w = np.ones([X.shape[1],1])\n",
    "        losses = []\n",
    "        y_predicted = np.zeros(len(X))\n",
    "        for i in range(len(X)):\n",
    "            y_predicted[i] = X[i]@w\n",
    "            loss = y[i] - y_predicted[i]\n",
    "            \n",
    "            print('the true value = ',y[i])\n",
    "            print('the predicted value = ',y_predicted[i])\n",
    "            losses.append(loss)\n",
    "        total_loss = np.sum(losses)\n",
    "        print('the total loss =', total_loss )\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = linear_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the true value =  5\n",
      "the predicted value =  5.0\n",
      "the true value =  9\n",
      "the predicted value =  9.0\n",
      "the true value =  13\n",
      "the predicted value =  13.0\n",
      "the true value =  14\n",
      "the predicted value =  14.0\n",
      "the true value =  15\n",
      "the predicted value =  15.0\n",
      "the true value =  16\n",
      "the predicted value =  16.0\n",
      "the true value =  17\n",
      "the predicted value =  17.0\n",
      "the true value =  18\n",
      "the predicted value =  18.0\n",
      "the true value =  19\n",
      "the predicted value =  19.0\n",
      "the true value =  20\n",
      "the predicted value =  20.0\n",
      "the total loss = 0.0\n"
     ]
    }
   ],
   "source": [
    "lm.train(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note that we used the correct w for this case and hence we have loss of zero, let's change ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class linear_model:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def train(self,X,y):\n",
    "        w = np.ones([X.shape[1],1])\n",
    "        w[0] = 0.2\n",
    "        losses = []\n",
    "        y_predicted = np.zeros(len(X))\n",
    "        for i in range(len(X)):\n",
    "            y_predicted[i] = X[i]@w\n",
    "            loss_at_w = y[i] - y_predicted[i]\n",
    "            loss_at_w_plus_h = y[i] - y_predicted[i]\n",
    "            \n",
    "            print('the true value = ',y[i])\n",
    "            print('the predicted value = ',y_predicted[i])\n",
    "            losses.append(loss_at_w )\n",
    "        total_loss = np.sum(losses)\n",
    "        print('the total loss =', total_loss)\n",
    "              \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = linear_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the true value =  5\n",
      "the predicted value =  4.2\n",
      "the true value =  9\n",
      "the predicted value =  7.4\n",
      "the true value =  13\n",
      "the predicted value =  10.6\n",
      "the true value =  14\n",
      "the predicted value =  11.6\n",
      "the true value =  15\n",
      "the predicted value =  12.6\n",
      "the true value =  16\n",
      "the predicted value =  13.6\n",
      "the true value =  17\n",
      "the predicted value =  14.6\n",
      "the true value =  18\n",
      "the predicted value =  15.6\n",
      "the true value =  19\n",
      "the predicted value =  16.6\n",
      "the true value =  20\n",
      "the predicted value =  17.6\n",
      "the total loss = 21.599999999999998\n"
     ]
    }
   ],
   "source": [
    "lm.train(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as you can see here, the loss is no longer 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it is obvious that we need to keep tweeking ws until we get the minium loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### now comes the calculs part, first we know that we can write the loss as a function of ws\n",
    "\n",
    "- L = f(w)\n",
    "- dl/dw = f'(w) = (f(w+h) - f(w))/h\n",
    "\n",
    "new_w = old+w - step_size* dl/dw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    " #ok, I need to update w which is a vector of shape (x[1],1)\n",
    "        # any change in w, for example if we change w[0], the vector y will change completely, not only y[0] as we dicussed before.\n",
    "        # now as we said we want to change w by tweeking it a little bit, by adding dw, i.e adding a vector that will be similar in length to w\n",
    "              \n",
    "        # for example :\n",
    "              # w = [1               df/dw = [0.10                                                        [1.2\n",
    "              #      2                     0.04            > new w will be old_w _ step_size*dw.     >. 2.08         note that here we assumed step*size to be 2\n",
    "               #     3]                    0.03]                                                         2.06\n",
    "        \n",
    "        # let's calculate dl/dw\n",
    "        # but this will involve calculating the loss twice, one at and then at w + h\n",
    "        # let's make a function of calculating the loss\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using a simple model to solve the equation 3X = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class linear_model:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def calc_loss(self,X,y,w):\n",
    "       \n",
    "        losses = []\n",
    "        y_predicted = np.zeros(len(X))\n",
    "       \n",
    "        \n",
    "        y_predicted = X@w\n",
    "        loss= y - y_predicted\n",
    "            \n",
    "        print('the true value = ',y)\n",
    "        print('the predicted value = ',y_predicted)\n",
    "        losses.append(loss)\n",
    "        total_loss = np.sum(losses)\n",
    "        print('the total loss =', total_loss )\n",
    "        return total_loss\n",
    "      \n",
    "\n",
    "\n",
    "    def train(self,X,y):\n",
    "        w = np.ones([X.shape[1],1])\n",
    "        w[0] = 5\n",
    "        \n",
    "        for i in range(len(w)):\n",
    "            loss_w = self.calc_loss(X,y,w)\n",
    "            \n",
    "            # the case when the loss is positive, in this case we need to decrease w\n",
    "            \n",
    "            # we will choose the margin that we will accept as 0.5\n",
    "            \n",
    "            if abs(loss_w)> 0:\n",
    "                # since the equation is 3x =6 , then dw/dl = 3\n",
    "                dldw = 3\n",
    "            elif abs(loss_w)< 0:\n",
    "                dldw = -3\n",
    "                # we will make the the learning rate is 0.5\n",
    "            lr =1\n",
    "            w[0]= w[0] -lr*dldw\n",
    "            loss_w_dash= self.calc_loss(X,y,w)\n",
    "            print(loss_w_dash - loss_w)\n",
    "            loss_w = loss_w_dash\n",
    "\n",
    "            \n",
    "            self.w = w\n",
    "            print(self.w)\n",
    "        \n",
    "    def predict(self,X_new):\n",
    "        predicted_y = X_new@self.w\n",
    "        \n",
    "        return predicted_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([3]).reshape(1,1)\n",
    "y = np.array([6]).reshape(1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.ones([1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.]])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X@w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = linear_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the true value =  [[6]]\n",
      "the predicted value =  [[15.]]\n",
      "the total loss = -9.0\n",
      "the true value =  [[6]]\n",
      "the predicted value =  [[6.]]\n",
      "the total loss = 0.0\n",
      "9.0\n",
      "[[2.]]\n"
     ]
    }
   ],
   "source": [
    "lm.train(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# so essentially the equation we have is\n",
    "\n",
    "# 2*X, this is our model, because this is basically what got us 6\n",
    "# let's see what if we try a new X, say for example 4, we expect the new y to be 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = np.array([4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = lm.predict(X_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([8.])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### let's now try equations for 2 variables\n",
    "\n",
    "\n",
    "- X1 = 2\n",
    "- X2 = 3\n",
    "\n",
    "- 2X1 + X2 = 7\n",
    "- 3X1 - X2 = 3\n",
    "\n",
    "> this is the equivelant of having 2 examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2,  1],\n",
       "       [ 3, -1]])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array([[2,1],\n",
    "             [3,-1]])\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[7],\n",
       "       [3]])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = np.array([7,3]).reshape(2,1)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "class linear_model:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def calc_loss(self,X,y,w):\n",
    "       \n",
    "        losses = []\n",
    "        y_predicted = np.zeros(len(X))\n",
    "       \n",
    "        \n",
    "        y_predicted = X@w\n",
    "        loss= y - y_predicted\n",
    "\n",
    "        print('the true value = ',y)\n",
    "        print('the predicted value = ',y_predicted)\n",
    "        losses.append(loss)\n",
    "        total_loss = np.sum(losses)\n",
    "        print('the total loss =', total_loss )\n",
    "        return total_loss\n",
    "      \n",
    "\n",
    "\n",
    "    def train(self,X,y):\n",
    "        w = np.ones([X.shape[1],1])\n",
    "        w[0] = 5\n",
    "        w[1] = 3\n",
    "        \n",
    "        loss_w = self.calc_loss(X,y,w)\n",
    "        while abs(loss_w) > 1:\n",
    "            for i in range(len(w)):\n",
    "                loss_w = self.calc_loss(X,y,w)\n",
    "\n",
    "                # the case when the loss is positive, in this case we need to decrease w\n",
    "\n",
    "                # we will choose the margin that we will accept as 0.5\n",
    "\n",
    "                if abs(loss_w)> 0:\n",
    "                    # since the equation is 3x =6 , then dw/dl = 3\n",
    "                    dldw = 1\n",
    "                elif abs(loss_w)< 0:\n",
    "                    dldw = -3\n",
    "                    # we will make the the learning rate is 0.5\n",
    "                lr =1\n",
    "                w[i]= w[i] -lr*dldw\n",
    "                loss_w_dash= self.calc_loss(X,y,w)\n",
    "                print(loss_w_dash - loss_w)\n",
    "                loss_w = loss_w_dash\n",
    "            \n",
    "            if abs(loss_w) < 5:\n",
    "                \n",
    "                self.w = w\n",
    "                \n",
    "                return self.w\n",
    "                print('converged to solution')\n",
    "                \n",
    "            else:\n",
    "                print('not converged')\n",
    "            \n",
    "            \n",
    "\n",
    "    def predict(self,X_new):\n",
    "        predicted_y = X_new@self.w\n",
    "        \n",
    "        return predicted_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = linear_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the true value =  [[7]\n",
      " [3]]\n",
      "the predicted value =  [[13.]\n",
      " [12.]]\n",
      "the total loss = -15.0\n",
      "the true value =  [[7]\n",
      " [3]]\n",
      "the predicted value =  [[13.]\n",
      " [12.]]\n",
      "the total loss = -15.0\n",
      "the true value =  [[7]\n",
      " [3]]\n",
      "the predicted value =  [[11.]\n",
      " [ 9.]]\n",
      "the total loss = -10.0\n",
      "5.0\n",
      "the true value =  [[7]\n",
      " [3]]\n",
      "the predicted value =  [[11.]\n",
      " [ 9.]]\n",
      "the total loss = -10.0\n",
      "the true value =  [[7]\n",
      " [3]]\n",
      "the predicted value =  [[10.]\n",
      " [10.]]\n",
      "the total loss = -10.0\n",
      "0.0\n",
      "not converged\n",
      "the true value =  [[7]\n",
      " [3]]\n",
      "the predicted value =  [[10.]\n",
      " [10.]]\n",
      "the total loss = -10.0\n",
      "the true value =  [[7]\n",
      " [3]]\n",
      "the predicted value =  [[8.]\n",
      " [7.]]\n",
      "the total loss = -5.0\n",
      "5.0\n",
      "the true value =  [[7]\n",
      " [3]]\n",
      "the predicted value =  [[8.]\n",
      " [7.]]\n",
      "the total loss = -5.0\n",
      "the true value =  [[7]\n",
      " [3]]\n",
      "the predicted value =  [[7.]\n",
      " [8.]]\n",
      "the total loss = -5.0\n",
      "0.0\n",
      "not converged\n",
      "the true value =  [[7]\n",
      " [3]]\n",
      "the predicted value =  [[7.]\n",
      " [8.]]\n",
      "the total loss = -5.0\n",
      "the true value =  [[7]\n",
      " [3]]\n",
      "the predicted value =  [[5.]\n",
      " [5.]]\n",
      "the total loss = 0.0\n",
      "5.0\n",
      "the true value =  [[7]\n",
      " [3]]\n",
      "the predicted value =  [[5.]\n",
      " [5.]]\n",
      "the total loss = 0.0\n",
      "the true value =  [[7]\n",
      " [3]]\n",
      "the predicted value =  [[4.]\n",
      " [6.]]\n",
      "the total loss = 0.0\n",
      "0.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[2.],\n",
       "       [0.]])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.train(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# even though this is not the correct value, but the tota loss is actually zero. I need to to solve this proble,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "#n the issue of losses cancelling themselves are a direct result of not taking the sum of the absolute value , which is L1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# L1 loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the true value =  [[7]\n",
      " [3]]\n",
      "the predicted value =  [[13.]\n",
      " [12.]]\n",
      "the total loss = -15.0\n",
      "the true value =  [[7]\n",
      " [3]]\n",
      "the predicted value =  [[13.]\n",
      " [12.]]\n",
      "the total loss = -15.0\n",
      "the true value =  [[7]\n",
      " [3]]\n",
      "the predicted value =  [[11.]\n",
      " [ 9.]]\n",
      "the total loss = -10.0\n",
      "5.0\n",
      "the true value =  [[7]\n",
      " [3]]\n",
      "the predicted value =  [[11.]\n",
      " [ 9.]]\n",
      "the total loss = -10.0\n",
      "the true value =  [[7]\n",
      " [3]]\n",
      "the predicted value =  [[10.]\n",
      " [10.]]\n",
      "the total loss = -10.0\n",
      "0.0\n",
      "not converged\n",
      "the true value =  [[7]\n",
      " [3]]\n",
      "the predicted value =  [[10.]\n",
      " [10.]]\n",
      "the total loss = -10.0\n",
      "the true value =  [[7]\n",
      " [3]]\n",
      "the predicted value =  [[8.]\n",
      " [7.]]\n",
      "the total loss = -5.0\n",
      "5.0\n",
      "the true value =  [[7]\n",
      " [3]]\n",
      "the predicted value =  [[8.]\n",
      " [7.]]\n",
      "the total loss = -5.0\n",
      "the true value =  [[7]\n",
      " [3]]\n",
      "the predicted value =  [[7.]\n",
      " [8.]]\n",
      "the total loss = -5.0\n",
      "0.0\n",
      "not converged\n",
      "the true value =  [[7]\n",
      " [3]]\n",
      "the predicted value =  [[7.]\n",
      " [8.]]\n",
      "the total loss = -5.0\n",
      "the true value =  [[7]\n",
      " [3]]\n",
      "the predicted value =  [[5.]\n",
      " [5.]]\n",
      "the total loss = 0.0\n",
      "5.0\n",
      "the true value =  [[7]\n",
      " [3]]\n",
      "the predicted value =  [[5.]\n",
      " [5.]]\n",
      "the total loss = 0.0\n",
      "the true value =  [[7]\n",
      " [3]]\n",
      "the predicted value =  [[4.]\n",
      " [6.]]\n",
      "the total loss = 0.0\n",
      "0.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[2.],\n",
       "       [0.]])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.train(X,y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# wa hoooooooooo :)\n",
    "\n",
    "- The breakthrough was the fact hat I have to calculate dwdl once\n",
    "- Also making the condition of stopping not zero but at 0.05\n",
    "- And playing with the learning rate\n",
    "- Of course also th earlier realization that I need to calucalte L1 and not just sum th loss since they were cancelling each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch vs stochastic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "while I was sleeping yesterday I was thinking about the calculation of gradients. More specifically I was thinking about how it is possible to calculate the gradient using one example, using some examples or all examples,or using all examples. The only difference will be in the way the loss is calculated. If only one example is included then we cacluate one loss, if n the the loss vectors, will have n rows each is associated with a loss, of course since we need only one number we will just take the mean of all losses. Note that we are calculatin the mean squared error using this methodology MAE. If we use L2. then we will be calculting the MSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "class linear_model:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def calc_loss(self,X,y,w, loss_measure = 'MSE'):\n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "        Calculating the loss or the deviation of the results we got from the model compared with the real values. The first step is to calculate the X@w which represents the results we got from\n",
    "        the model. The second step is to evaulate the loss which can be done through a loss function that we define. for example here we define two ways to caculate the loss either using MAE, in\n",
    "        which we first calculate the absolute difference between each predicted and true value and then we calucalte the mean. or using the MSW, in which we calculate the abolute difference between\n",
    "        true and preidcted value, square it and then find the average. The loss we then be returned.\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "       \n",
    "        losses = []\n",
    "        y_predicted = np.zeros(len(X))\n",
    "        y_predicted = X@w\n",
    "        \n",
    "        if loss_measure == 'MSE':\n",
    "            loss = (y - y_predicted)**2\n",
    "        elif loss_measure == 'MAE':\n",
    "            loss= abs(y - y_predicted)\n",
    "\n",
    "        #print('the true value = ',y)\n",
    "        #print('the predicted value = ',y_predicted)\n",
    "        losses.append(loss)\n",
    "        total_loss = np.mean(losses)\n",
    "        #print('the total loss =', total_loss )\n",
    "        return total_loss\n",
    "      \n",
    "\n",
    "\n",
    "    def train(self,X,y,loss_measure = 'MAE'):\n",
    "        #w = np.ones([X.shape[1],1])\n",
    "        np.random.seed(500)\n",
    "        w = np.random.rand(X.shape[1],1)\n",
    "        dldw = np.ones([X.shape[1],1])\n",
    "        dldw[0] = 0.02\n",
    "        dldw[1] = 0.02\n",
    "        w[0] = 3\n",
    "        w[1] = 3\n",
    "        print(w)\n",
    "        \n",
    "    # calculate the derivitives, this happen once, that was my mistake, I was including it in the for loop.\n",
    "\n",
    "        for i in range(len(w)):\n",
    "\n",
    "            # f(w)\n",
    "\n",
    "            loss_w = self.calc_loss(X,y,w,loss_measure = loss_measure)\n",
    "            # we will choose the margin that we will accept as 0.5\n",
    "\n",
    "            # f(w+h)\n",
    "\n",
    "            h = 0.005\n",
    "            w[i] = w[i] + h\n",
    "\n",
    "            # dldw = f(w+h) - f(w)/h\n",
    "            dldw[i] = (self.calc_loss(X,y,w,loss_measure = loss_measure) - loss_w)/h\n",
    "\n",
    "\n",
    "        print(dldw)   \n",
    "        \n",
    "        # updating the weight\n",
    "        \n",
    "        lr =0.005\n",
    "        \n",
    "        for i in range(20):\n",
    "            print('step',i)\n",
    "            loss_w = self.calc_loss(X,y,w)\n",
    "            print('loss is now',loss_w)\n",
    "            if loss_w <= 0.08 :\n",
    "                self.w = w\n",
    "                print('final w = ',self.w)\n",
    "                return self.w\n",
    "            elif loss_w > 0.08 :\n",
    "                w = w - lr*dldw\n",
    "#         for i in range(10):\n",
    "#             counter = counter + 1\n",
    "            \n",
    "#             print('loss = ',loss_w_dash)\n",
    "#             #print(loss_w_dash - loss_w)\n",
    "#             loss_w = loss_w_dash\n",
    "\n",
    "#             \n",
    "\n",
    "#                 return self.w\n",
    "                \n",
    "                \n",
    "\n",
    "    def predict(self,X_new):\n",
    "        predicted_y = X_new@self.w\n",
    "        \n",
    "        return predicted_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3.]\n",
      " [3.]]\n",
      "[[13.0325]\n",
      " [-1.    ]]\n",
      "step 0\n",
      "loss is now 6.560162500000003\n",
      "step 1\n",
      "loss is now 5.7321160466406065\n",
      "step 2\n",
      "loss is now 4.9599711865624725\n",
      "step 3\n",
      "loss is now 4.243727919765584\n",
      "step 4\n",
      "loss is now 3.5833862462499555\n",
      "step 5\n",
      "loss is now 2.978946166015569\n",
      "step 6\n",
      "loss is now 2.4304076790624403\n",
      "step 7\n",
      "loss is now 1.9377707853905637\n",
      "step 8\n",
      "loss is now 1.501035484999939\n",
      "step 9\n",
      "loss is now 1.1202017778905646\n",
      "step 10\n",
      "loss is now 0.7952696640624431\n",
      "step 11\n",
      "loss is now 0.5262391435155735\n",
      "step 12\n",
      "loss is now 0.31311021624995605\n",
      "step 13\n",
      "loss is now 0.15588288226559058\n",
      "step 14\n",
      "loss is now 0.05455714156247709\n",
      "final w =  [[2.092725]\n",
      " [3.075   ]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[2.092725],\n",
       "       [3.075   ]])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm = linear_model()\n",
    "lm.train(X,y,loss_measure = 'MSE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_2d_new = np.array([[2,3],\n",
    "                   [0,5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[13.41045],\n",
       "       [15.375  ]])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.predict(X_2d_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's use the model we have trained for predicting the results for new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The effect of initialzing weighhts with very smalll values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ok, we know that dldw is basically how much the loss is affected when the weight is changed a little bit. Since the trajectory is not linear\n",
    "# we expect that dldw is not constant. But I basically makes it so in the above code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think now I started to grasp the idea of foward pass and backward pass.\n",
    "Where do we need the dldw, we need it when we want to update w. Now dldw is not constatnt because again , imagin that you are in this half-circle \n",
    "down. as you apprach the bottom the slopes acually goes dowm. I am going to do this for the above code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "class linear_model:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "\n",
    "    \n",
    "    def calc_loss(self,X,y,w, loss_measure = 'MSE'):\n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "        Calculating the loss or the deviation of the results we got from the model compared with the real values. The first step is to calculate the X@w which represents the results we got from\n",
    "        the model. The second step is to evaulate the loss which can be done through a loss function that we define. for example here we define two ways to caculate the loss either using MAE, in\n",
    "        which we first calculate the absolute difference between each predicted and true value and then we calucalte the mean. or using the MSW, in which we calculate the abolute difference between\n",
    "        true and preidcted value, square it and then find the average. The loss we then be returned.\n",
    "        \n",
    "        \"\"\"    \n",
    "        losses = []\n",
    "        y_predicted = np.zeros(len(X))\n",
    "        y_predicted = X@w\n",
    "        \n",
    "        if loss_measure == 'MSE':\n",
    "            loss = (y - y_predicted)**2\n",
    "        elif loss_measure == 'MAE':\n",
    "            loss= abs(y - y_predicted)\n",
    "\n",
    "        #print('the true value = ',y)\n",
    "        #print('the predicted value = ',y_predicted)\n",
    "        losses.append(loss)\n",
    "        total_loss = np.mean(losses)\n",
    "        #print('the total loss =', total_loss )\n",
    "        return total_loss\n",
    "      \n",
    "\n",
    "    def diff(self,w,X,y, loss_measure,diff_type = 'Analytical'):\n",
    "        \"\"\"\n",
    "        given a function f(x) and a point w, calculate th f'(x)\n",
    "        \n",
    "        \"\"\"\n",
    "        dldw = np.ones([X.shape[1],1])\n",
    "        h = 0.005  \n",
    "        \n",
    "        if diff_type =='Numerical':\n",
    "            # calculate the derivitives dl/dw using limits\n",
    "            for i in range(len(w)):\n",
    "                print('i',i)\n",
    "                # f(w)\n",
    "                loss_w = self.calc_loss(X,y,w,loss_measure = loss_measure)\n",
    "                # we will choose the margin that we will accept as 0.5\n",
    "                # f(w+h)\n",
    "                w[i] = w[i] + h      \n",
    "                loss_w_plus_h = self.calc_loss(X,y,w,loss_measure = loss_measure)\n",
    "                # dldw = f(w+h) - f(w)/h\n",
    "                dldw[i] = (loss_w_plus_h- loss_w)/h\n",
    "                print('dldw using numerical solution')\n",
    "                print(dldw)\n",
    "        elif diff_type == 'Analytical':\n",
    "            # calculate the derivitves dl/dw using calculus\n",
    "            # l = (X.w - y)**2\n",
    "            # dl/dw = 2*(X.w - y)@X = 2*X@()\n",
    "\n",
    "            dldw = 2*X@(X@w - y)\n",
    "            print('dldw using analytical solution')\n",
    "            print(dldw)\n",
    "        return dldw \n",
    "    \n",
    "    def train(self,X,y,loss_measure = 'MAE', lr = 0.07, diff_type = 'Analytical'):\n",
    "        \"\"\"\n",
    "        training of the model is done by evaluting loss and updating the weights\n",
    "        \"\"\"\n",
    "        # initilization\n",
    "        np.random.seed(50)\n",
    "        w = np.random.rand(X.shape[1],1)\n",
    "        # intialization. I am changing this manually\n",
    "        #w[0] = \n",
    "        #w[1] = 3.5\n",
    "        print('initial weights')\n",
    "        print(w)    \n",
    "        # updating the weight     \n",
    "        #lr =0.07\n",
    "        \n",
    "        for i in range(30):\n",
    "            print('step =',i)\n",
    "            \n",
    "            # forward pass is essentially the process of calculting the loss given the data and the weight, for the loss we need to calculate a predicted value\n",
    "            # calculate the loss at the current weights\n",
    "            loss_w = self.calc_loss(X,y,w)\n",
    "            print('loss is now',loss_w)\n",
    "            # stoping condition, note that we don't require the loss is to be exactly zero but close enough\n",
    "            if loss_w <= 0.08 :\n",
    "                self.w = w\n",
    "                print('final w = ',self.w)\n",
    "                return self.w\n",
    "            \n",
    "            # updating the weight. \n",
    "            elif loss_w > 0.08 :\n",
    "                # the backpropagation is essentially the calculation of the partially derivives using chain rules. since we have only one layer at this time we will just calculate the dldw\n",
    "                #First at that particular point we calculate dldw \n",
    "                dldw = self.diff(w,X,y,loss_measure, diff_type)\n",
    "                # Now we update the weight\n",
    "                w = w -lr*dldw\n",
    "                print('w')\n",
    "                print(w)          \n",
    "    def predict(self,X_new):\n",
    "        \"\"\"\n",
    "        Given a new data, the model will be using the weights stored in the self.w to produce a prediction\n",
    "        \"\"\"\n",
    "        predicted_y = X_new@self.w\n",
    "        \n",
    "        return predicted_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial weights\n",
      "[[0.49460165]\n",
      " [0.2280831 ]]\n",
      "step = 0\n",
      "loss is now 18.241141479081318\n",
      "dldw using analytical solution\n",
      "[[-26.61941075]\n",
      " [-31.20772529]]\n",
      "w\n",
      "[[2.62415451]\n",
      " [2.72470113]]\n",
      "step = 1\n",
      "loss is now 2.7798160070416325\n",
      "dldw using analytical solution\n",
      "[[8.18756534]\n",
      " [1.54253606]]\n",
      "w\n",
      "[[1.96914928]\n",
      " [2.60129824]]\n",
      "step = 2\n",
      "loss is now 0.1528493396307682\n",
      "dldw using analytical solution\n",
      "[[-1.22931361]\n",
      " [-3.37471838]]\n",
      "w\n",
      "[[2.06749437]\n",
      " [2.87127571]]\n",
      "step = 3\n",
      "loss is now 0.054868788984643146\n",
      "final w =  [[2.06749437]\n",
      " [2.87127571]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[2.06749437],\n",
       "       [2.87127571]])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm = linear_model()\n",
    "lm.train(X,y,loss_measure = 'MSE', lr = 0.08,diff_type = 'Analytical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[12.74881588],\n",
       "       [14.35637857]])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.predict(X_2d_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my thought process during building the model from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- note that weight in the first dimention X[0] is being updated correctly, when I initialize with weight that is bigger than the correct once, the weight goes down, and when I initialize with \n",
    "a weight that is smaller than the correct weight then the weight will go up. But, for some reason there is almost no movement in the second dimention, X[1], the quetion is why ?I alreay tried initializing with a different set of values, but still there is no movements.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- I guess the solution deponds on how X1 and X2 affect f(x) and hence the loss function. It seems that if we change w a bit in X1 direction, the loss will be affected significantly, but if we \n",
    "change w in the X2 direction, then the loss will be affected that much hence the slow conversion to the correct values of w."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More than one Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "class linear_model:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    def sigmoid(self,z):\n",
    "        return 1/(1+np.exp(-z))\n",
    "\n",
    "    def sigmoid_grad(self,z):\n",
    "        return sigmoid(z) * 1-sigmoid(z)\n",
    "    \n",
    "    \n",
    "    def calc_loss(self,X,y,w, loss_measure = 'MSE'):\n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "        Calculating the loss or the deviation of the results we got from the model compared with the real values. The first step is to calculate the X@w which represents the results we got from\n",
    "        the model. The second step is to evaulate the loss which can be done through a loss function that we define. for example here we define two ways to caculate the loss either using MAE, in\n",
    "        which we first calculate the absolute difference between each predicted and true value and then we calucalte the mean. or using the MSW, in which we calculate the abolute difference between\n",
    "        true and preidcted value, square it and then find the average. The loss we then be returned.\n",
    "        \n",
    "        \"\"\"    \n",
    "        losses = []\n",
    "        z = X@w\n",
    "        #y_predicted = z\n",
    "        y_predicted = self.sigmoid(z)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        if loss_measure == 'MSE':\n",
    "            loss = (y - y_predicted)**2\n",
    "        elif loss_measure == 'MAE':\n",
    "            loss= abs(y - y_predicted)\n",
    "\n",
    "        #print('the true value = ',y)\n",
    "        #print('the predicted value = ',y_predicted)\n",
    "        losses.append(loss)\n",
    "        total_loss = np.mean(losses)\n",
    "        #print('the total loss =', total_loss )\n",
    "        return total_loss\n",
    "      \n",
    "\n",
    "    def diff(self,w,X,y, loss_measure,diff_type = 'Analytical'):\n",
    "        \"\"\"\n",
    "        given a function f(x) and a point w, calculate th f'(x)\n",
    "        \n",
    "        \"\"\"\n",
    "        dldw = np.ones([X.shape[1],1])\n",
    "        h = 0.005  \n",
    "        \n",
    "        if diff_type =='Numerical':\n",
    "            # calculate the derivitives dl/dw using limits\n",
    "            for i in range(len(w)):\n",
    "                print('i',i)\n",
    "                # f(w)\n",
    "                loss_w = self.calc_loss(X,y,w,loss_measure = loss_measure)\n",
    "                # we will choose the margin that we will accept as 0.5\n",
    "                # f(w+h)\n",
    "                w[i] = w[i] + h      \n",
    "                loss_w_plus_h = self.calc_loss(X,y,w,loss_measure = loss_measure)\n",
    "                # dldw = f(w+h) - f(w)/h\n",
    "                dldw[i] = (loss_w_plus_h- loss_w)/h\n",
    "                print('dldw using numerical solution')\n",
    "                print(dldw)\n",
    "        elif diff_type == 'Analytical':\n",
    "            # calculate the derivitves dl/dw using calculus\n",
    "            # l = (X.w - y)**2\n",
    "            # dl/dw = 2*(X.w - y)@X = 2*X@()\n",
    "\n",
    "            dldw = 2*X@(X@w - y)\n",
    "            print('dldw using analytical solution')\n",
    "            print(dldw)\n",
    "        return dldw \n",
    "    \n",
    "    def train(self,X,y,loss_measure = 'MAE', lr = 0.07, diff_type = 'Analytical'):\n",
    "        \"\"\"\n",
    "        training of the model is done by evaluting loss and updating the weights\n",
    "        \"\"\"\n",
    "        # initilization\n",
    "        np.random.seed(50)\n",
    "        w = np.random.rand(X.shape[1],1)\n",
    "        # intialization. I am changing this manually\n",
    "        #w[0] = \n",
    "        #w[1] = 3.5\n",
    "        print('initial weights')\n",
    "        print(w)    \n",
    "        # updating the weight     \n",
    "        #lr =0.07\n",
    "        \n",
    "        for i in range(30):\n",
    "            print('step =',i)\n",
    "            \n",
    "            # forward pass is essentially the process of calculting the loss given the data and the weight, for the loss we need to calculate a predicted value\n",
    "            # calculate the loss at the current weights\n",
    "            loss_w = self.calc_loss(X,y,w)\n",
    "            print('loss is now',loss_w)\n",
    "            # stoping condition, note that we don't require the loss is to be exactly zero but close enough\n",
    "            if loss_w <= 0.08 :\n",
    "                self.w = w\n",
    "                print('final w = ',self.w)\n",
    "                return self.w\n",
    "            \n",
    "            # updating the weight. \n",
    "            elif loss_w > 0.08 :\n",
    "                # the backpropagation is essentially the calculation of the partially derivives using chain rules. since we have only one layer at this time we will just calculate the dldw\n",
    "                #First at that particular point we calculate dldw \n",
    "                dldw = self.diff(w,X,y,loss_measure, diff_type)\n",
    "                # Now we update the weight\n",
    "                w = w -lr*dldw\n",
    "                print('w')\n",
    "                print(w)          \n",
    "    def predict(self,X_new):\n",
    "        \"\"\"\n",
    "        Given a new data, the model will be using the weights stored in the self.w to produce a prediction\n",
    "        \"\"\"\n",
    "        predicted_y = X_new@self.w\n",
    "        \n",
    "        return predicted_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial weights\n",
      "[[0.49460165]\n",
      " [0.2280831 ]]\n",
      "step = 0\n",
      "loss is now 21.86457304319812\n",
      "dldw using analytical solution\n",
      "[[-26.61941075]\n",
      " [-31.20772529]]\n",
      "w\n",
      "[[2.62415451]\n",
      " [2.72470113]]\n",
      "step = 1\n",
      "loss is now 20.013641498984054\n",
      "dldw using analytical solution\n",
      "[[8.18756534]\n",
      " [1.54253606]]\n",
      "w\n",
      "[[1.96914928]\n",
      " [2.60129824]]\n",
      "step = 2\n",
      "loss is now 20.08000579699743\n",
      "dldw using analytical solution\n",
      "[[-1.22931361]\n",
      " [-3.37471838]]\n",
      "w\n",
      "[[2.06749437]\n",
      " [2.87127571]]\n",
      "step = 3\n",
      "loss is now 20.075060231466576\n",
      "dldw using analytical solution\n",
      "[[ 0.68747257]\n",
      " [-0.62482808]]\n",
      "w\n",
      "[[2.01249656]\n",
      " [2.92126196]]\n",
      "step = 4\n",
      "loss is now 20.091554636108313\n",
      "dldw using analytical solution\n",
      "[[ 0.01747578]\n",
      " [-0.55492495]]\n",
      "w\n",
      "[[2.0110985 ]\n",
      " [2.96565596]]\n",
      "step = 5\n",
      "loss is now 20.09544591859577\n",
      "dldw using analytical solution\n",
      "[[ 0.0866909 ]\n",
      " [-0.20816136]]\n",
      "w\n",
      "[[2.00416323]\n",
      " [2.98230886]]\n",
      "step = 6\n",
      "loss is now 20.098741593338783\n",
      "dldw using analytical solution\n",
      "[[ 0.02290291]\n",
      " [-0.11654972]]\n",
      "w\n",
      "[[2.00233099]\n",
      " [2.99163284]]\n",
      "step = 7\n",
      "loss is now 20.100052674760263\n",
      "dldw using analytical solution\n",
      "[[ 0.01589961]\n",
      " [-0.0529513 ]]\n",
      "w\n",
      "[[2.00105903]\n",
      " [2.99586895]]\n",
      "step = 8\n",
      "loss is now 20.100780395207867\n",
      "dldw using analytical solution\n",
      "[[ 0.00656425]\n",
      " [-0.02669428]]\n",
      "w\n",
      "[[2.00053389]\n",
      " [2.99800449]]\n",
      "step = 9\n",
      "loss is now 20.101115976167115\n",
      "dldw using analytical solution\n",
      "[[ 0.00348337]\n",
      " [-0.01276078]]\n",
      "w\n",
      "[[2.00025522]\n",
      " [2.99902535]]\n",
      "step = 10\n",
      "loss is now 20.10128477107667\n",
      "dldw using analytical solution\n",
      "[[ 0.00162372]\n",
      " [-0.0062659 ]]\n",
      "w\n",
      "[[2.00012532]\n",
      " [2.99952662]]\n",
      "step = 11\n",
      "loss is now 20.101365763327166\n",
      "dldw using analytical solution\n",
      "[[ 0.0008077 ]\n",
      " [-0.00303511]]\n",
      "w\n",
      "[[2.0000607 ]\n",
      " [2.99976943]]\n",
      "step = 12\n",
      "loss is now 20.101405508121182\n",
      "dldw using analytical solution\n",
      "[[ 0.00038869]\n",
      " [-0.00148033]]\n",
      "w\n",
      "[[2.00002961]\n",
      " [2.99988786]]\n",
      "step = 13\n",
      "loss is now 20.101424777961373\n",
      "dldw using analytical solution\n",
      "[[ 0.00019021]\n",
      " [-0.00071949]]\n",
      "w\n",
      "[[2.00001439]\n",
      " [2.99994542]]\n",
      "step = 14\n",
      "loss is now 20.101434174960538\n",
      "dldw using analytical solution\n",
      "[[ 9.22937282e-05]\n",
      " [-3.50318582e-04]]\n",
      "w\n",
      "[[2.00000701]\n",
      " [2.99997344]]\n",
      "step = 15\n",
      "loss is now 20.10143874329717\n",
      "dldw using analytical solution\n",
      "[[ 4.49757258e-05]\n",
      " [-1.70415679e-04]]\n",
      "w\n",
      "[[2.00000341]\n",
      " [2.99998708]]\n",
      "step = 16\n",
      "loss is now 20.10144096749816\n",
      "dldw using analytical solution\n",
      "[[ 2.18694216e-05]\n",
      " [-8.29379929e-05]]\n",
      "w\n",
      "[[2.00000166]\n",
      " [2.99999371]]\n",
      "step = 17\n",
      "loss is now 20.101442049544726\n",
      "dldw using analytical solution\n",
      "[[ 1.06457483e-05]\n",
      " [-4.03549998e-05]]\n",
      "w\n",
      "[[2.00000081]\n",
      " [2.99999694]]\n",
      "step = 18\n",
      "loss is now 20.10144257614901\n",
      "dldw using analytical solution\n",
      "[[ 5.17931017e-06]\n",
      " [-1.96377591e-05]]\n",
      "w\n",
      "[[2.00000039]\n",
      " [2.99999851]]\n",
      "step = 19\n",
      "loss is now 20.10144283238175\n",
      "dldw using analytical solution\n",
      "[[ 2.52052424e-06]\n",
      " [-9.55566216e-06]]\n",
      "w\n",
      "[[2.00000019]\n",
      " [2.99999928]]\n",
      "step = 20\n",
      "loss is now 20.10144295707063\n",
      "dldw using analytical solution\n",
      "[[ 1.22644303e-06]\n",
      " [-4.64989002e-06]]\n",
      "w\n",
      "[[2.00000009]\n",
      " [2.99999965]]\n",
      "step = 21\n",
      "loss is now 20.101443017744003\n",
      "dldw using analytical solution\n",
      "[[ 5.96809239e-07]\n",
      " [-2.26265306e-06]]\n",
      "w\n",
      "[[2.00000005]\n",
      " [2.99999983]]\n",
      "step = 22\n",
      "loss is now 20.10144304726831\n",
      "dldw using analytical solution\n",
      "[[ 2.90407376e-07]\n",
      " [-1.10102354e-06]]\n",
      "w\n",
      "[[2.00000002]\n",
      " [2.99999992]]\n",
      "step = 23\n",
      "loss is now 20.101443061634953\n",
      "dldw using analytical solution\n",
      "[[ 1.41314882e-07]\n",
      " [-5.35764013e-07]]\n",
      "w\n",
      "[[2.00000001]\n",
      " [2.99999996]]\n",
      "step = 24\n",
      "loss is now 20.10144306862587\n",
      "dldw using analytical solution\n",
      "[[ 6.87644555e-08]\n",
      " [-2.60706189e-07]]\n",
      "w\n",
      "[[2.00000001]\n",
      " [2.99999998]]\n",
      "step = 25\n",
      "loss is now 20.101443072027685\n",
      "dldw using analytical solution\n",
      "[[ 3.34612551e-08]\n",
      " [-1.26861165e-07]]\n",
      "w\n",
      "[[2.        ]\n",
      " [2.99999999]]\n",
      "step = 26\n",
      "loss is now 20.10144307368303\n",
      "dldw using analytical solution\n",
      "[[ 1.62824350e-08]\n",
      " [-6.17314218e-08]]\n",
      "w\n",
      "[[2.]\n",
      " [3.]]\n",
      "step = 27\n",
      "loss is now 20.101443074488532\n",
      "dldw using analytical solution\n",
      "[[ 7.92313415e-09]\n",
      " [-3.00388834e-08]]\n",
      "w\n",
      "[[2.]\n",
      " [3.]]\n",
      "step = 28\n",
      "loss is now 20.101443074880496\n",
      "dldw using analytical solution\n",
      "[[ 3.85544396e-09]\n",
      " [-1.46171022e-08]]\n",
      "w\n",
      "[[2.]\n",
      " [3.]]\n",
      "step = 29\n",
      "loss is now 20.101443075071227\n",
      "dldw using analytical solution\n",
      "[[ 1.87608595e-09]\n",
      " [-7.11276815e-09]]\n",
      "w\n",
      "[[2.]\n",
      " [3.]]\n"
     ]
    }
   ],
   "source": [
    "lm = linear_model()\n",
    "lm.train(X,y,loss_measure = 'MSE', lr = 0.08,diff_type = 'Analytical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it's amazing how we got exactly the right answer just by applying the sigmoid function and even wituout including the derivitive of sigmoid in the backpropagation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2,  1],\n",
       "       [ 3, -1]])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[7],\n",
       "       [3]])"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2X1 + X2 = 7\n",
    "\n",
    "3X1 - X2 = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X1 = 2, X2 = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, what if X1 =4, X2= 6\n",
    "\n",
    "then y = [14,6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "y[0][0] = 14\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "y[1][0] = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2,  1],\n",
       "       [ 3, -1]])"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[14],\n",
       "       [ 6]])"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial weights\n",
      "[[0.49460165]\n",
      " [0.2280831 ]]\n",
      "step = 0\n",
      "loss is now 101.1286071748941\n",
      "dldw using analytical solution\n",
      "[[-60.61941075]\n",
      " [-67.20772529]]\n",
      "w\n",
      "[[5.34415451]\n",
      " [5.60470113]]\n",
      "step = 1\n",
      "loss is now 97.00014908356842\n",
      "dldw using analytical solution\n",
      "[[18.02756534]\n",
      " [ 4.90253606]]\n",
      "w\n",
      "[[3.90194928]\n",
      " [5.21249824]]\n",
      "step = 2\n",
      "loss is now 97.00758596749306\n",
      "dldw using analytical solution\n",
      "[[-2.94771361]\n",
      " [-6.88831838]]\n",
      "w\n",
      "[[4.13776637]\n",
      " [5.76356371]]\n",
      "step = 3\n",
      "loss is now 97.0064746876639\n",
      "dldw using analytical solution\n",
      "[[ 1.45585657]\n",
      " [-1.06489208]]\n",
      "w\n",
      "[[4.02129784]\n",
      " [5.84875508]]\n",
      "step = 4\n",
      "loss is now 97.00998879944528\n",
      "dldw using analytical solution\n",
      "[[-0.00432006]\n",
      " [-1.08217231]]\n",
      "w\n",
      "[[4.02164345]\n",
      " [5.93532886]]\n",
      "step = 5\n",
      "loss is now 97.01087699175652\n",
      "dldw using analytical solution\n",
      "[[ 0.17366598]\n",
      " [-0.3875084 ]]\n",
      "w\n",
      "[[4.00775017]\n",
      " [5.96632954]]\n",
      "step = 6\n",
      "loss is now 97.0116943837374\n",
      "dldw using analytical solution\n",
      "[[ 0.04116143]\n",
      " [-0.22286269]]\n",
      "w\n",
      "[[4.00445725]\n",
      " [5.98415855]]\n",
      "step = 7\n",
      "loss is now 97.01202179760583\n",
      "dldw using analytical solution\n",
      "[[ 0.03071866]\n",
      " [-0.09998805]]\n",
      "w\n",
      "[[4.00199976]\n",
      " [5.9921576 ]]\n",
      "step = 8\n",
      "loss is now 97.01220740791618\n",
      "dldw using analytical solution\n",
      "[[ 0.01231185]\n",
      " [-0.05074066]]\n",
      "w\n",
      "[[4.00101481]\n",
      " [5.99621685]]\n",
      "step = 9\n",
      "loss is now 97.01229304400893\n",
      "dldw using analytical solution\n",
      "[[ 0.00664108]\n",
      " [-0.02417632]]\n",
      "w\n",
      "[[4.00048353]\n",
      " [5.99815096]]\n",
      "step = 10\n",
      "loss is now 97.0123363460273\n",
      "dldw using analytical solution\n",
      "[[ 0.00307128]\n",
      " [-0.0118912 ]]\n",
      "w\n",
      "[[4.00023782]\n",
      " [5.99910225]]\n",
      "step = 11\n",
      "loss is now 97.0123571228022\n",
      "dldw using analytical solution\n",
      "[[ 0.00153404]\n",
      " [-0.00575505]]\n",
      "w\n",
      "[[4.0001151 ]\n",
      " [5.99956265]]\n",
      "step = 12\n",
      "loss is now 97.01236733176773\n",
      "dldw using analytical solution\n",
      "[[ 0.00073672]\n",
      " [-0.00280815]]\n",
      "w\n",
      "[[4.00005616]\n",
      " [5.99978731]]\n",
      "step = 13\n",
      "loss is now 97.01237228128923\n",
      "dldw using analytical solution\n",
      "[[ 0.0003609 ]\n",
      " [-0.00136456]]\n",
      "w\n",
      "[[4.00002729]\n",
      " [5.99989647]]\n",
      "step = 14\n",
      "loss is now 97.01237469571868\n",
      "dldw using analytical solution\n",
      "[[ 0.00017502]\n",
      " [-0.00066447]]\n",
      "w\n",
      "[[4.00001329]\n",
      " [5.99994963]]\n",
      "step = 15\n",
      "loss is now 97.01237586947097\n",
      "dldw using analytical solution\n",
      "[[ 8.53131031e-05]\n",
      " [-3.23221179e-04]]\n",
      "w\n",
      "[[4.00000646]\n",
      " [5.99997549]]\n",
      "step = 16\n",
      "loss is now 97.01237644098532\n",
      "dldw using analytical solution\n",
      "[[ 4.14778163e-05]\n",
      " [-1.57309914e-04]]\n",
      "w\n",
      "[[4.00000315]\n",
      " [5.99998807]]\n",
      "step = 17\n",
      "loss is now 97.01237671901848\n",
      "dldw using analytical solution\n",
      "[[ 2.01922483e-05]\n",
      " [-7.65409208e-05]]\n",
      "w\n",
      "[[4.00000153]\n",
      " [5.9999942 ]]\n",
      "step = 18\n",
      "loss is now 97.01237685433277\n",
      "dldw using analytical solution\n",
      "[[ 9.82347754e-06]\n",
      " [-3.72470107e-05]]\n",
      "w\n",
      "[[4.00000074]\n",
      " [5.99999718]]\n",
      "step = 19\n",
      "loss is now 97.01237692017327\n",
      "dldw using analytical solution\n",
      "[[ 4.78070440e-06]\n",
      " [-1.81241931e-05]]\n",
      "w\n",
      "[[4.00000036]\n",
      " [5.99999863]]\n",
      "step = 20\n",
      "loss is now 97.01237695221295\n",
      "dldw using analytical solution\n",
      "[[ 2.32618636e-06]\n",
      " [-8.81944761e-06]]\n",
      "w\n",
      "[[4.00000018]\n",
      " [5.99999933]]\n",
      "step = 21\n",
      "loss is now 97.01237696780342\n",
      "dldw using analytical solution\n",
      "[[ 1.13196926e-06]\n",
      " [-4.29157059e-06]]\n",
      "w\n",
      "[[4.00000009]\n",
      " [5.99999967]]\n",
      "step = 22\n",
      "loss is now 97.01237697538988\n",
      "dldw using analytical solution\n",
      "[[ 5.50814979e-07]\n",
      " [-2.08831066e-06]]\n",
      "w\n",
      "[[4.00000004]\n",
      " [5.99999984]]\n",
      "step = 23\n",
      "loss is now 97.01237697908152\n",
      "dldw using analytical solution\n",
      "[[ 2.68031902e-07]\n",
      " [-1.01618303e-06]]\n",
      "w\n",
      "[[4.00000002]\n",
      " [5.99999992]]\n",
      "step = 24\n",
      "loss is now 97.0123769808779\n",
      "dldw using analytical solution\n",
      "[[ 1.30425443e-07]\n",
      " [-4.94481215e-07]]\n",
      "w\n",
      "[[4.00000001]\n",
      " [5.99999996]]\n",
      "step = 25\n",
      "loss is now 97.01237698175201\n",
      "dldw using analytical solution\n",
      "[[ 6.34659347e-08]\n",
      " [-2.40617455e-07]]\n",
      "w\n",
      "[[4.        ]\n",
      " [5.99999998]]\n",
      "step = 26\n",
      "loss is now 97.01237698217736\n",
      "dldw using analytical solution\n",
      "[[ 3.08828856e-08]\n",
      " [-1.17085930e-07]]\n",
      "w\n",
      "[[4.        ]\n",
      " [5.99999999]]\n",
      "step = 27\n",
      "loss is now 97.01237698238434\n",
      "dldw using analytical solution\n",
      "[[ 1.50278083e-08]\n",
      " [-5.69747165e-08]]\n",
      "w\n",
      "[[4.]\n",
      " [6.]]\n",
      "step = 28\n",
      "loss is now 97.01237698248505\n",
      "dldw using analytical solution\n",
      "[[ 7.31261096e-09]\n",
      " [-2.77242478e-08]]\n",
      "w\n",
      "[[4.]\n",
      " [6.]]\n",
      "step = 29\n",
      "loss is now 97.01237698253406\n",
      "dldw using analytical solution\n",
      "[[ 3.5583696e-09]\n",
      " [-1.3490780e-08]]\n",
      "w\n",
      "[[4.]\n",
      " [6.]]\n"
     ]
    }
   ],
   "source": [
    "lm = linear_model()\n",
    "lm.train(X,y,loss_measure = 'MSE', lr = 0.08,diff_type = 'Analytical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# well I again got the correct answer, I should get some time to think about this mistery?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
